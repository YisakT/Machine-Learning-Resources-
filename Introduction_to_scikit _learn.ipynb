{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f5621fa",
   "metadata": {},
   "source": [
    "# Introduction to Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29d6c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " # This notebook demonstrates some of the most useful functions of the beautiful Scikit-Learn Library.\n",
    "\n",
    "What_we_are_going_to_cover = [\n",
    "\n",
    "\"0. An end -to-end Scikit learn workfolw.\",\n",
    "\"1. Getting the data ready.\",\n",
    "\"2. Choose the right estimator/algorith for our problems.\",\n",
    "\"3. Fit the model/algorithm and use it to make predictions on our data\",\n",
    "\"4. Evaluate a model.\",\n",
    "\"5. Improve a model.\",\n",
    "\"6. Save and Load a trained model.\",\n",
    "\"7. Putting it all together!\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c816c214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0. An end -to-end Scikit learn workfolw.',\n",
       " '1. Getting the data ready.',\n",
       " '2. Choose the right estimator/algorith for our problems.',\n",
       " '3. Fit the model/algorithm and use it to make predictions on our data',\n",
       " '4. Evaluate a model.',\n",
       " '5. Improve a model.',\n",
       " '6. Save and Load a trained model.',\n",
       " '7. Putting it all together!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "What_we_are_going_to_cover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ad16e",
   "metadata": {},
   "source": [
    "## 0.An end-to-end Scikit-Learn Workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "532b7b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0     63    1   3       145   233    1        0      150      0      2.3   \n",
       "1     37    1   2       130   250    0        1      187      0      3.5   \n",
       "2     41    0   1       130   204    0        0      172      0      1.4   \n",
       "3     56    1   1       120   236    0        1      178      0      0.8   \n",
       "4     57    0   0       120   354    0        1      163      1      0.6   \n",
       "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       "298   57    0   0       140   241    0        1      123      1      0.2   \n",
       "299   45    1   3       110   264    0        1      132      0      1.2   \n",
       "300   68    1   0       144   193    1        1      141      0      3.4   \n",
       "301   57    1   0       130   131    0        1      115      1      1.2   \n",
       "302   57    0   1       130   236    0        0      174      0      0.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "0        0   0     1       1  \n",
       "1        0   0     2       1  \n",
       "2        2   0     2       1  \n",
       "3        2   0     2       1  \n",
       "4        2   0     2       1  \n",
       "..     ...  ..   ...     ...  \n",
       "298      1   0     3       0  \n",
       "299      1   0     3       0  \n",
       "300      1   2     3       0  \n",
       "301      1   1     3       0  \n",
       "302      1   1     2       0  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Get the data ready\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "heart_disease = pd.read_csv(\"heart-disease.csv\")\n",
    "heart_disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d576585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X (features matrix) \n",
    "\n",
    "# In summary, you are splitting your dataset into two parts:\n",
    "\n",
    "   # X, which contains all the independent variables (features) without the target column.\n",
    "   # Y, which contains only the target column.\n",
    "\n",
    "#This is a standard procedure in preparing data for supervised learning, where X is used to train the model, \n",
    "# and Y is what the model attempts to predict.\n",
    "# This separation is a common practice when preparing data for machine learning, \n",
    "# as it allows you to train your model to learn the relationship between the input features (X) \n",
    "# and the target output (Y).\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "\n",
    "# Create y (Labels)\n",
    "Y = heart_disease[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d872696f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Choose the right model and hyperparameters\n",
    "# clf is short for classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf  = RandomForestClassifier()\n",
    "\n",
    "# we'll keep the default hyperparameters\n",
    "\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7798fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 . Fit the model to the training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "# Importing the train_test_split function\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# This line imports the train_test_split function from the model_selection module of scikit-learn.\n",
    "# The train_test_split function is a utility that helps you easily split your dataset\n",
    "# into a training set and a testing set.\n",
    "\n",
    "# Splitting the Dataset into Training and Testing Sets\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "# This line is where the actual splitting of the data occurs.\n",
    "# train_test_split(X, Y, test_size=0.2) takes your features matrix X and your labels Y and splits \n",
    "# them into training and testing sets.\n",
    "# The test_size=0.2 argument specifies that 20% of the data should be set aside for testing.\n",
    "# This means that 80% of the data will be used for training the model.\n",
    "# The function returns four subsets:\n",
    "#    - X_train: The subset of your features used for training the model.\n",
    "#    - X_test: The subset of your features used for testing the model.\n",
    "#    - Y_train: The subset of your labels corresponding to X_train, used for training the model.\n",
    "#    - Y_test: The subset of your labels corresponding to X_test, used for evaluating the model's performance.\n",
    "\n",
    "# In simple terms, this process divides your dataset into two parts: \n",
    "# one part to train your machine learning model (the training set) and \n",
    "# another part to test its performance and see how well it generalizes to new, unseen data (the testing set).\n",
    "# This is a fundamental practice in machine learning to avoid overfitting, \n",
    "# where a model performs well on the training data but poorly on new data.\n",
    "# By evaluating the model on a separate testing set, \n",
    "# you get a better sense of its real-world performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a2479e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, Y_train);\n",
    "\n",
    "# Fitting the model to the training data\n",
    "\n",
    "# clf.fit(X_train, Y_train)\n",
    "# This line is where the model 'learns' from the data.\n",
    "# The 'fit' method is used to train the model using the training data.\n",
    "# 'clf' is the instance of RandomForestClassifier we created earlier.\n",
    "\n",
    "# X_train: This is the training data (features), which the model uses to learn.\n",
    "# Y_train: These are the true labels for the training data.\n",
    "\n",
    "# During the fitting process, the RandomForestClassifier will look at the X_train and Y_train data,\n",
    "# and try to figure out the patterns or relationships between the features and the target label.\n",
    "# This process involves the RandomForestClassifier building multiple decision trees,\n",
    "# each looking at different aspects and combinations of the data,\n",
    "# and then combining their insights to make more accurate predictions.\n",
    "\n",
    "# Once the model is fitted, it can then be used to make predictions on new, unseen data.\n",
    "# The 'fit' method is one of the most fundamental and first steps in the model building process \n",
    "# in machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96bfc365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yisak\\OneDrive\\Desktop\\machine learning projects\\env\\lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0. 2. 3. 4.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Make a prediction\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m Y_label \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\machine learning projects\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:823\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    803\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 823\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    826\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\machine learning projects\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:865\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    863\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    868\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\machine learning projects\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:599\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    598\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 599\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\machine learning projects\\env\\lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\machine learning projects\\env\\lib\\site-packages\\sklearn\\utils\\validation.py:940\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 940\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    941\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    942\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    943\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    944\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    945\u001b[0m         )\n\u001b[0;32m    947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    949\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    950\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    951\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0. 2. 3. 4.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Make a prediction\n",
    "Y_label = clf.predict(np.array([0,2,3,4]))\n",
    "\n",
    "# this fails because the shape of the imput is not corect.\n",
    "# we are puting in a array.\n",
    "\n",
    "\n",
    "\n",
    "# Y_label = clf.predict(np.array([0,2,3,4]))\n",
    "# This line is attempting to use the trained model to make a prediction.\n",
    "# 'clf.predict()' is the method used to predict the label of new data using the trained model.\n",
    "\n",
    "# However, this line raises an error because the shape of the input data does not match the \n",
    "# shape the model expects.\n",
    "# The issue here is that we are passing a 1-dimensional array as input, \n",
    "# whereas the model expects the input to have the same number of features as the training data (X_train).\n",
    "\n",
    "# In this case, the model was trained on a certain number of features (columns in X_train),\n",
    "# but the input provided is a single array with only 4 values.\n",
    "# Each value in this array is being interpreted as a separate input example rather than a set of \n",
    "# features for a single example.\n",
    "\n",
    "# To fix this, the input data needs to be reshaped or reformatted to match the format of the training data.\n",
    "# If you're trying to predict for one example, the input should be a 2-dimensional array \n",
    "# with one row for the example and columns matching the number of features the model was trained on.\n",
    "\n",
    "# An example correction could be to reshape the array to have 1 row and the correct number of columns:\n",
    "# Y_label = clf.predict(np.array([[0, 2, 3, 4]]))\n",
    "# Note: The inner brackets create a 2D array with one row and four columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a4c37-ffa5-4e75-ab46-26e28f6ab9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the Error in Prediction Attempt\n",
    "\n",
    "# Error Message: \n",
    "# \"Expected 2D array, got 1D array instead: array=[0. 2. 3. 4.]. \n",
    "# Reshape your data either using array.reshape(-1, 1) if your data has a single feature \n",
    "# or array.reshape(1, -1) if it contains a single sample.\"\n",
    "\n",
    "# This error occurs because the input data provided to clf.predict() is not in the correct format.\n",
    "# The RandomForestClassifier model expects the input data to be a 2-dimensional array, \n",
    "# but the provided data is a 1-dimensional array.\n",
    "\n",
    "# In the context of the model, a 2D array represents a collection of samples, \n",
    "# where each sample has multiple features. \n",
    "# The model is trained on such an array, so it expects the same format for making predictions.\n",
    "\n",
    "# To fix this error, the input data needs to be reshaped into a 2D array. \n",
    "# Since the intention is to predict a single sample with multiple features, \n",
    "# you should reshape the array to have 1 row (representing 1 sample) and multiple columns \n",
    "# (representing features).\n",
    "\n",
    "# The correct way to reshape and make a prediction would be:\n",
    "# Y_label = clf.predict(np.array([0, 2, 3, 4]).reshape(1, -1))\n",
    "# Here, .reshape(1, -1) changes the shape of the array to have 1 row and as many columns as\n",
    "# necessary to accommodate the data.\n",
    "\n",
    "# This reshaping ensures that the data format matches what the model expects, \n",
    "# allowing the prediction method to work correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8ae5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it has to look like this for it to work\n",
    "# has to be a 2d array.\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_preds = clf.predict(X_test)\n",
    "Y_preds\n",
    "\n",
    "# Making Predictions on the Test Set\n",
    "\n",
    "# Y_preds = clf.predict(X_test)\n",
    "# This line uses the trained RandomForestClassifier (clf) to make predictions on the test data (X_test).\n",
    "# The 'predict' method of clf is used to predict the labels for each sample in X_test.\n",
    "\n",
    "# X_test contains the features of the unseen test data. \n",
    "# This data was set aside during the train-test split and was not used in training the model.\n",
    "# The model will use the patterns it learned during training to predict the labels for this new data.\n",
    "\n",
    "# The predictions made by the model are stored in the variable Y_preds.\n",
    "# Y_preds will be a numpy array containing the predicted labels for each sample in X_test.\n",
    "\n",
    "# Y_preds\n",
    "# This line when executed in a Jupyter Notebook will display the contents of Y_preds.\n",
    "# It shows the predictions made by the model for the test set.\n",
    "# These predicted labels can be compared with the actual labels (Y_test) to evaluate the model's performance.\n",
    "\n",
    "# By comparing Y_preds with Y_test, you can assess how well your model is performing.\n",
    "# Common ways to evaluate classification models include accuracy, precision, recall, and the confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f8dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate the Model\n",
    "clf.score(X_train, Y_train)\n",
    "\n",
    "# Evaluating the Model on the Training Set\n",
    "\n",
    "# clf.score(X_train, Y_train)\n",
    "# This line evaluates the performance of your RandomForestClassifier model on the training data.\n",
    "# The 'score' method returns the accuracy of the model, which is the proportion of correct predictions.\n",
    "\n",
    "# X_train and Y_train are the features and labels of the training set, respectively.\n",
    "# The model was trained on this data, so this score tells you how well the model fits the training data.\n",
    "\n",
    "# However, it's important to note that evaluating the model on the training data can be misleading.\n",
    "# A high score on the training data might simply mean that the model has memorized the \n",
    "# training data (overfitting),\n",
    "# rather than learning the underlying patterns in the data.\n",
    "\n",
    "# A more accurate assessment of the model's performance is obtained by evaluating it on the \n",
    "# test set (X_test and Y_test),\n",
    "# which consists of data that the model hasn't seen during training.\n",
    "# This helps in understanding how well the model generalizes to new, unseen data.\n",
    "\n",
    "# Generally, in machine learning, it's recommended to look at both the training score and the test score.\n",
    "# A good model will have high scores on both the training and testing datasets.\n",
    "# If the model performs well on the training data but poorly on the test data, \n",
    "# it's an indication that the model may be overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9198b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test,Y_test)\n",
    "\n",
    "# Evaluating the Model on the Test Set\n",
    "\n",
    "# clf.score(X_test, Y_test)\n",
    "# This line evaluates the performance of your RandomForestClassifier model on the test data.\n",
    "# The 'score' method, in this case, returns the accuracy of the model for the test set,\n",
    "# which is the proportion of correct predictions out of all predictions made on the test set.\n",
    "\n",
    "# X_test and Y_test are the features and labels of the test set, respectively.\n",
    "# The test set is crucial for evaluating the model because it consists of data that the model\n",
    "# has not seen during training.\n",
    "# This helps in understanding how well the model generalizes to new, unseen data.\n",
    "\n",
    "# A high score on the test set indicates that the model not only learned the patterns in the training data,\n",
    "# but it is also able to apply these patterns to make accurate predictions on new data.\n",
    "\n",
    "# It's important to compare the model's performance on the training set (evaluated previously) and the test set.\n",
    "# Ideally, the model should perform well on both sets. \n",
    "# A large discrepancy between training and test scores might indicate issues such as overfitting \n",
    "# (if the training score is much higher) \n",
    "# or underfitting (if the training score is too low compared to the test score).\n",
    "\n",
    "# The accuracy obtained here gives a quick overview of how well the model performs, \n",
    "# but it's also useful to look at other metrics like precision, recall, F1 score, and confusion matrices\n",
    "# for a more comprehensive evaluation, especially in cases where the dataset is imbalanced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More ways to evaluate the model.\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
    "\n",
    "# This below will return the compararison of the test label and the predication label.\n",
    "print(classification_report(Y_test, Y_preds))\n",
    "\n",
    "\n",
    "# Generating a Classification Report for Model Evaluation\n",
    "\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "# This line imports various evaluation metrics from scikit-learn's metrics module,\n",
    "# including classification_report, which we'll use to evaluate the model.\n",
    "\n",
    "# Using classification_report to Evaluate the Model\n",
    "# print(classification_report(Y_test, Y_preds))\n",
    "# This line generates and prints out the classification report for your model based on the test data.\n",
    "# The classification report provides detailed performance metrics for each class.\n",
    "\n",
    "# Y_test: These are the true labels from the test set.\n",
    "# Y_preds: These are the predicted labels by your model.\n",
    "\n",
    "# The classification report includes several important metrics:\n",
    "# - Precision: Measures the accuracy of positive predictions for each class.\n",
    "# - Recall: Measures the ability of the model to find all the positive samples for each class.\n",
    "# - F1-score: Provides a balance between precision and recall. It's a harmonic mean of the two.\n",
    "# - Support: Indicates the number of actual occurrences of each class in the specified dataset.\n",
    "\n",
    "# These metrics are provided for each class in your target variable and include averages,\n",
    "# giving you a detailed overview of how well the model is performing for each type of classification.\n",
    "# This detailed breakdown can help in identifying if the model is underperforming for any particular class \n",
    "# and assist in further refining the model or addressing any data imbalances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290302b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test,Y_preds)\n",
    "\n",
    "# Generating a Confusion Matrix for Model Evaluation\n",
    "\n",
    "# confusion_matrix(Y_test, Y_preds)\n",
    "# This line generates the confusion matrix for your model based on the test data.\n",
    "# A confusion matrix is a table often used to describe the performance of a classification model.\n",
    "\n",
    "# Y_test: These are the true labels from the test set.\n",
    "# Y_preds: These are the predicted labels by your model.\n",
    "\n",
    "# The confusion matrix compares the actual target values with those predicted by the model,\n",
    "# providing a detailed breakdown of:\n",
    "# - True Positives (TP): Correctly predicted positive observations\n",
    "# - True Negatives (TN): Correctly predicted negative observations\n",
    "# - False Positives (FP): Incorrectly predicted positive observations (Type I error)\n",
    "# - False Negatives (FN): Incorrectly predicted negative observations (Type II error)\n",
    "\n",
    "# Each row of the matrix represents the instances in an actual class, \n",
    "# while each column represents the instances in a predicted class, or vice versa. \n",
    "# This setup allows you to see the types of errors (if any) your model is making.\n",
    "\n",
    "# Interpreting the confusion matrix can provide insights into not only the overall performance of the model\n",
    "# but also into how it performs on each individual class. It can be particularly useful to identify \n",
    "# any biases the model may have towards certain classes and can inform how you might improve the model,\n",
    "# perhaps by providing more data for underrepresented classes or by tweaking the model itself.\n",
    "\n",
    "# Generally, a high number of True Positives and True Negatives and low numbers of False Positives \n",
    "# and False Negatives\n",
    "# are indicative of good model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f145b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y_test,Y_preds)\n",
    "\n",
    "# Calculating the Accuracy Score for Model Evaluation\n",
    "\n",
    "# accuracy_score(Y_test, Y_preds)\n",
    "# This line calculates the accuracy of the model based on the test data.\n",
    "# Accuracy is one of the most common metrics used to evaluate classification models.\n",
    "\n",
    "# Y_test: These are the true labels from the test set.\n",
    "# Y_preds: These are the predicted labels by your model.\n",
    "\n",
    "# The accuracy score is the ratio of correct predictions to total predictions made:\n",
    "# Accuracy = (True Positives + True Negatives) / Total Predictions\n",
    "\n",
    "# It represents how often the classifier is correct overall across all classes.\n",
    "# In simple terms, it answers the question, \"Out of all the classifications, how many did the model get right?\"\n",
    "\n",
    "# While accuracy is a useful metric, it should be considered alongside other metrics like precision, recall, \n",
    "# and the F1 score, especially in scenarios where the data is imbalanced or when different types of \n",
    "# errors have different costs.\n",
    "\n",
    "# A high accuracy score indicates that the model has a high rate of correctly predicting both\n",
    "# positive and negative classes.\n",
    "# However, don't rely solely on accuracy if the dataset is imbalanced (i.e., one class is much \n",
    "# more frequent than others).\n",
    "# In such cases, the model might just predict the most common class most of the time and still \n",
    "# achieve high accuracy.\n",
    "\n",
    "# Typically, after getting an overall sense of the model's performance through accuracy, \n",
    "# you would dive deeper into the performance details using the confusion matrix and classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533fd091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Improve the Model.\n",
    "# Try diferent amounts of the n_estimators\n",
    "np.random.seed(42)\n",
    "for i in range (10,100,10):\n",
    "    print(f\"Trying model with {i} estimators....\")\n",
    "    clf = RandomForestClassifier(n_estimators=i).fit(X_train,Y_train)\n",
    "    print(f\"Model accuracy on the test set: {clf.score(X_test,Y_test) * 100:.2f}%\")\n",
    "    print(\"\")      \n",
    "\n",
    "# Improving the Model by Tuning n_estimators in RandomForestClassifier\n",
    "\n",
    "# Trying different amounts of n_estimators\n",
    "# np.random.seed(42)\n",
    "# Setting a random seed ensures the results are reproducible.\n",
    "\n",
    "# for i in range(10, 100, 10):\n",
    "# This loop iterates through different values for 'n_estimators' starting from 10 up to 90, \n",
    "# increasing by 10 each time.\n",
    "# 'n_estimators' in a RandomForestClassifier refers to the number of trees in the forest.\n",
    "\n",
    "# print(f\"Trying model with {i} estimators....\")\n",
    "# This prints out the number of estimators the model is using in the current iteration.\n",
    "\n",
    "# clf = RandomForestClassifier(n_estimators=i).fit(X_train, Y_train)\n",
    "# Here, a new RandomForestClassifier is created with the current number of estimators (i) \n",
    "# and fitted to the training data.\n",
    "# This is done within the loop, so the classifier is retrained for each different value of 'n_estimators'.\n",
    "\n",
    "# print(f\"Model accuracy on the test set: {clf.score(X_test, Y_test) * 100:.2f}%\")\n",
    "# After fitting the model, this line prints out the accuracy of the classifier on the test data.\n",
    "# Multiplying by 100 converts it into a percentage, and :.2f formats the number to two decimal places.\n",
    "\n",
    "# print(\"\")  \n",
    "# This just prints a new line for better readability between each iteration's results.\n",
    "\n",
    "# By iterating through different values of 'n_estimators', you can observe how changing the number of trees\n",
    "# impacts the model's accuracy on the test set. \n",
    "# The goal is to identify the number of trees that provides the best trade-off between performance\n",
    "# and computational efficiency.\n",
    "\n",
    "# It's important to note that more trees in the forest doesn't always mean a better model. \n",
    "# After a certain point, increasing the number of trees may not significantly improve the \n",
    "# model's performance and can even \n",
    "# lead to longer training times. This experiment helps to find an optimal point or at least a\n",
    "# range of 'n_estimators' \n",
    "# that offers good model accuracy without unnecessary computational cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Save a model and load it\n",
    "import pickle\n",
    "\n",
    "pickle.dump(clf, open(\"random_forest_model_1.pk1\", \"wb\"))\n",
    "\n",
    "# Saving the Trained Model for Later Use\n",
    "\n",
    "# import pickle\n",
    "# 'pickle' is a Python module used to serialize and deserialize Python objects.\n",
    "# Serialization is the process of converting a Python object into a byte stream,\n",
    "# and deserialization is the reverse process. It's useful for saving models and other data structures.\n",
    "\n",
    "# pickle.dump(clf, open(\"random_forest_model_1.pk1\", \"wb\"))\n",
    "# This line saves the trained RandomForestClassifier (clf) to a file.\n",
    "\n",
    "# clf: This is the trained RandomForestClassifier model that you want to save.\n",
    "\n",
    "# open(\"random_forest_model_1.pk1\", \"wb\"): This opens a file named 'random_forest_model_1.pk1'\n",
    "#  in binary write mode ('wb').\n",
    "# If the file doesn't exist, it will be created. If it does exist, it will be overwritten.\n",
    "\n",
    "# The 'dump' function of pickle is used to serialize the 'clf' object and write it to the file.\n",
    "# This saved model can be loaded later to make predictions without needing to retrain the model.\n",
    "\n",
    "# Saving the model is particularly useful if the training process is time-consuming,\n",
    "# or if you want to deploy the model for use in applications, share it with others, or simply keep \n",
    "# a version of the model \n",
    "# that you can return to later.\n",
    "\n",
    "# Note: It's important to remember that the version of scikit-learn (or any other libraries used) \n",
    "# should be the same when you save the model and when you load it, as changes in the library versions \n",
    "# might not be compatible with the model file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92070459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets load the model. \"rb\" stands for read binaries\n",
    "loded_model = pickle.load(open(\"random_forest_model_1.pk1\", \"rb\"))\n",
    "\n",
    "# score the model .\n",
    "\n",
    "loded_model.score(X_test,Y_test)\n",
    "\n",
    "# Loading the Saved Model and Evaluating It\n",
    "\n",
    "# Loading the model\n",
    "# loded_model = pickle.load(open(\"random_forest_model_1.pk1\", \"rb\"))\n",
    "# This line loads the previously saved model into the variable 'loaded_model'.\n",
    "# 'pickle.load()' is used to deserialize the object, converting the byte stream back into a Python object.\n",
    "\n",
    "# open(\"random_forest_model_1.pk1\", \"rb\"): This opens the file named 'random_forest_model_1.pk1' \n",
    "# in binary read mode ('rb').\n",
    "# The file must exist in the directory you're working in, or you should provide the correct path to the file.\n",
    "\n",
    "# \"rb\" stands for \"read binary\", which is necessary because the model was saved in a binary format.\n",
    "\n",
    "# Once loaded, 'loaded_model' is essentially a clone of the 'clf' object that was saved earlier.\n",
    "# It retains all the properties, parameters, and learned patterns of the original trained model.\n",
    "\n",
    "# Scoring the loaded model\n",
    "# loaded_model.score(X_test, Y_test)\n",
    "# After loading the model, you might want to confirm that it's still performing as expected.\n",
    "# This line uses the 'score' method to evaluate the loaded model's accuracy, just like you did with \n",
    "# the original 'clf' model.\n",
    "\n",
    "# X_test and Y_test are the same test set features and labels used to evaluate the original model.\n",
    "# This line effectively gives you the accuracy of the model on the test set, \n",
    "# allowing you to verify that the model has been loaded correctly and is functioning as expected.\n",
    "\n",
    "# Using 'loaded_model.score()' is a quick way to ensure that the deserialization process \n",
    "# has worked correctly and that the model is ready to be used for predictions or further evaluation.\n",
    "\n",
    "# This process is particularly useful in operational settings where you might train a model in one\n",
    "# script or notebook,\n",
    "# save it, and then load it in a different script, application, or system to make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb192e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "What_we_are_going_to_cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c3201c",
   "metadata": {},
   "source": [
    "## 1. getting our data ready to be used with machine learning\n",
    "\n",
    "Three main things we have to do.\n",
    "\n",
    "1.Split the data into features and labels (usually 'x' and 'y')\n",
    "\n",
    "2.Filling (also called inputing) or disregarding missing values\n",
    "\n",
    "3.Converting non-numerical values to numerical values (also called feature encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c86f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc66cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets drop target or remove target from the table\n",
    "\n",
    "x=heart_disease.drop(\"target\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db542d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the y axis will be target \n",
    "y=heart_disease[\"target\"]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88212ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c9713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 242 came from spliting\n",
    "# 13 came from the number of coulmns\n",
    "x.shape[0] * 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707c558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(heart_disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d02595b",
   "metadata": {},
   "source": [
    "### 1.1  Make sure its all numerical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc1f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales =pd.read_csv(\"car-sales-extended.csv\")\n",
    "car_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a24db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(car_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96857b79-963c-434b-81c3-3cdcf01529cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7269f3b-6eeb-450d-8d97-4f2724357113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X/Y\n",
    "\n",
    "X = car_sales.drop(\"Price\", axis=1)\n",
    "Y = car_sales[\"Price\"]\n",
    "\n",
    "# Split into training and test\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7ad12-db4b-47f5-9475-c1c961d1443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build machine learning model\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, Y_train)\n",
    "model.score(X_test,Y_test)\n",
    "\n",
    "# will get Value error if you do not convert  strings into int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce914b8c-223b-4e40-b641-e0d4da422405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we convert our data in to numbers\n",
    "# Turn the catagories into numbers\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "catagorical_features = [\"Make\", \"Colour\",\"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\",one_hot,catagorical_features)],remainder=\"passthrough\")\n",
    "\n",
    "transformed_X = transformer.fit_transform(X)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84de715-4f84-474f-99e2-cc97aea5b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it in a data frame\n",
    "pd.DataFrame(transformed_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6554a7-e503-4b4e-9d22-e03218ba89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A second way to change to number\n",
    "\n",
    "dummies = pd.get_dummies(car_sales[[\"Make\", \"Colour\",\"Doors\"]])\n",
    "dummies \n",
    "\n",
    "# A second way to change to number\n",
    "\n",
    "# dummies = pd.get_dummies(car_sales[[\"Make\", \"Colour\",\"Doors\"]])\n",
    "# This line creates dummy variables for categorical data in the car_sales DataFrame.\n",
    "# 'pd.get_dummies()' is a function from the pandas library that converts categorical variable(s)\n",
    "# into dummy/indicator variables.\n",
    "\n",
    "# What are dummy variables?\n",
    "# - Dummy variables are binary (0 or 1) columns created to represent each unique value in a \n",
    "# categorical variable.\n",
    "# - For each unique category in the variable, a new column is created where 1 indicates the\n",
    "# presence of the category and 0 indicates the absence.\n",
    "\n",
    "# car_sales[[\"Make\", \"Colour\", \"Doors\"]]\n",
    "# - This part is selecting the 'Make', 'Colour', and 'Doors' columns from the car_sales DataFrame.\n",
    "# - These are typically categorical columns where 'Make' might be car brands, 'Colour' could be the \n",
    "# color of the car, \n",
    "#   and 'Doors' the number of doors (which might be treated as categorical, e.g., '2-door', '4-door', etc.).\n",
    "\n",
    "# dummies = ...\n",
    "# - The resulting dummy variables are stored in a new DataFrame called 'dummies'.\n",
    "# - This DataFrame now contains several columns, each representing a unique value from the original\n",
    "# columns as binary indicators.\n",
    "\n",
    "# dummies\n",
    "# - This line, when executed, displays the DataFrame 'dummies'.\n",
    "# - It's useful to visualize the new dummy variables to understand how the original categorical \n",
    "# data has been transformed.\n",
    "\n",
    "# Why use dummy variables?\n",
    "# - Many machine learning models require numerical input and do not handle categorical data natively.\n",
    "# - Dummy variables allow for the representation of categorical data in a binary, numerical format\n",
    "# that models can understand.\n",
    "# - It avoids the issue of assigning ordinal numbers to categories which might lead the model to assume \n",
    "# a natural order among categories.\n",
    "\n",
    "# Note: After creating dummy variables, it's important to consider multicollinearity, \n",
    "# which can occur due to the dummy variable trap (one dummy variable can be predicted from others).\n",
    "# A common practice is to drop one dummy column for each original categorical variable to avoid this issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97199cf-ad0a-407b-a4cb-81fc61364fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's refit the model\n",
    "\n",
    "np.random.seed(42)\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(transformed_X,Y, test_size=0.2)\n",
    "\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a45a92-ec49-4ebc-9112-be061bc10c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b792fda-ca90-4e24-8777-0ae74c506062",
   "metadata": {},
   "source": [
    "## 1.2  Dealing with missing data\n",
    "\n",
    "1. Fill them with some value (also known as imputaion)\n",
    "2.  Remove the sample with missing data altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e6991-a244-4812-a264-a5d9595b9b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import car sales missing data\n",
    "\n",
    "car_sales_missing = pd.read_csv(\"car-sales-extended-missing-data.csv\")\n",
    "car_sales_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663719fe-0112-4393-a2bd-d25dbe7c884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is intended to give you a summary count of all missing (NaN) \n",
    "# values in each column of the car_sales_missing DataFrame.\n",
    "\n",
    "car_sales_missing.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a92bc-248b-424a-9e8b-7f6662ce91f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X & Y\n",
    "\n",
    "x = car_sales_missing.drop(\"Price\", axis=1)\n",
    "y = car_sales_missing[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218c5d55-e560-4625-a8a1-50b4116dcbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let's try and conver them into numbers\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "catagorical_features = [\"Make\", \"Colour\",\"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\",one_hot,catagorical_features)],remainder=\"passthrough\")\n",
    "\n",
    "transformed_X = transformer.fit_transform(X)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d90eb2-508e-417b-ae05-cf930c0a1910",
   "metadata": {},
   "source": [
    "### 1: Fill Missing Data with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de39afb2-41f2-4b5b-85c7-02de3be86d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code is using the fillna() method from pandas to handle missing values in a DataFrame called car_sales_missing. Here's what each line is doing:\n",
    "\n",
    "# Filling \"Make\" Column: It replaces all missing (NaN) values in the \"Make\" column with the string \"missing\".\n",
    "\n",
    "# Filling \"Colour\" Column: It replaces all missing (NaN) values in the \"Colour\" column with the string \"missing\".\n",
    "\n",
    "# Filling \"Odometer (KM)\" Column: It replaces all missing (NaN) values in the \"Odometer (KM)\" column with \n",
    "# the mean (average) of the existing values in that column.\n",
    "\n",
    "# Filling \"Doors\" Column: It replaces all missing (NaN) values in the \"Doors\" column with the number 4.\n",
    "\n",
    "# In all cases, the inplace=True parameter updates the DataFrame directly, meaning it changes the original\n",
    "# car_sales_missing DataFrame rather than creating a new one. This is a common way to handle missing\n",
    "# data by assigning default or calculated values to ensure the dataset is complete for analysis \n",
    "# or further processing.\n",
    "\n",
    "\n",
    "# Fill the \"make \" column\n",
    "\n",
    "car_sales_missing[\"Make\"].fillna(\"missing\", inplace=True)\n",
    "\n",
    "# Fill the \"Colour\" column\n",
    "\n",
    "car_sales_missing[\"Colour\"].fillna(\"missing\", inplace=True)\n",
    "\n",
    "# Fill the \"Odometer (KM)\" column\n",
    "\n",
    "car_sales_missing[\"Odometer (KM)\"].fillna(car_sales_missing[\"Odometer (KM)\"].mean(), inplace=True)\n",
    "\n",
    "# Fill the \"Doors\" column\n",
    "\n",
    "car_sales_missing[\"Doors\"].fillna(4, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f653502c-4692-4942-b62f-137407c5e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our data frame again\n",
    "\n",
    "# Price column is left out in purpose as its the one we are trying to predict.\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1bb62-c176-4e2a-af90-4e42c3b66367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with missing price value\n",
    "car_sales_missing.dropna(inplace=True)\n",
    "#  The code car_sales_missing.dropna(inplace=True) removes all rows in the car_sales_missing \n",
    "# DataFrame that contain any missing values (NaN) and updates the DataFrame in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe6c89e-7a55-4992-ada7-840b7e9e4c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use .isna() and . sum() to get the sum of all missing values\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b700af39-028b-437e-85eb-77846893656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(car_sales_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f223b-51da-4eb3-b4c1-b3f6f60c2578",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = car_sales_missing.drop(\"Price\", axis=1)\n",
    "y = car_sales_missing[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23964a10-0424-4467-ba17-2ff443131818",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "catagorical_features = [\"Make\", \"Colour\",\"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\",one_hot,catagorical_features)],remainder=\"passthrough\")\n",
    "\n",
    "transformed_X = transformer.fit_transform(X)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a8a1d-86e9-4f3e-872d-589b0843f64e",
   "metadata": {},
   "source": [
    "### Option 2 : Fill missing values with Sickit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a33bc-3374-4138-a1c0-00329510a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataframe which has missing vales and inspect top five values.\n",
    "car_sales_missing = pd.read_csv(\"car-sales-extended-missing-data.csv\")\n",
    "car_sales_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb23a7f2-680c-4d80-a35e-647527dac156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use .isna() and . sum() to get the sum of all missing values\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3e95f-5da1-46d5-84cd-46e2f86d512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are dropping na values in the Price column \n",
    "# Then recaluculate how much are missing.\n",
    "car_sales_missing.dropna(subset=[\"Price\"], inplace=True)\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0768acec-e116-4f6e-a72e-346c2212b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into X & Y \n",
    "x = car_sales_missing.drop(\"Price\", axis=1)\n",
    "y = car_sales_missing[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977672e5-f6be-4efd-a0e5-cbf90cd933a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Imputer are used here to fill missing data\n",
    "# Setup imputers: one for categorical, one for door number, and one for numerical features\n",
    "cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"missing\")  # Fills missing categorical values with 'missing'\n",
    "door_imputer = SimpleImputer(strategy=\"constant\", fill_value=4)  # Fills missing door values with '4'\n",
    "num_imputer = SimpleImputer(strategy=\"mean\")  # Fills missing numerical values with mean of the column\n",
    "\n",
    "# Define feature lists for different data types\n",
    "cat_features = [\"Make\", \"Colour\"]  # Categorical feature names\n",
    "door_feature = [\"Doors\"]  # Door feature name\n",
    "num_features = [\"Odometer (KM)\"]  # Numerical feature names\n",
    "\n",
    "# Create a ColumnTransformer to apply each imputer to the correct features\n",
    "# ColumnTransformer takes a list of multiple transformers \n",
    "# Each element of the list is a tuple, specifying the transformer to apply to a specific subset of columns. \n",
    "# This is powerful because it allows different treatments for different types of data\n",
    "imputer = ColumnTransformer([\n",
    "    (\"cat_imputer\", cat_imputer, cat_features),  # Apply categorical imputer to cat_features\n",
    "    (\"door_imputer\", door_imputer, door_feature),  # Apply door imputer to door_feature\n",
    "    (\"num_imputer\", num_imputer, num_features)  # Apply numerical imputer to num_features\n",
    "])\n",
    "\n",
    "# Assume X is your dataframe\n",
    "# Apply the ColumnTransformer to the data\n",
    "filled_X = imputer.fit_transform(X)  # Imputes missing values in X\n",
    "filled_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4432e86-a7c8-4204-a93c-9c7c6fcf3c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the filled column \n",
    "car_sales_filled = pd.DataFrame(filled_X,\n",
    "                              columns=[\"Make\",\"Colour\",\"Doors\", \"Odometer (KM)\"])\n",
    "\n",
    "car_sales_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9def9b05-2d60-4bf8-8530-4b3a3ce664ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see here that we have no missing values\n",
    "car_sales_filled.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130450b5-f0c1-406d-9265-52fbea2126b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we convert our data in to numbers\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "catagorical_features = [\"Make\", \"Colour\",\"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\",one_hot,catagorical_features)],remainder=\"passthrough\")\n",
    "\n",
    "transformed_X = transformer.fit_transform(car_sales_filled)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2817cb9b-0040-4097-a47c-41fe3d3cdf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have filled missing data and convertedd to numbers \n",
    "# lets fit a model.\n",
    "\n",
    "np.random.seed(42)\n",
    "#This line sets the seed for NumPy's random number generator to '42', \n",
    "# ensuring that the results are reproducible.\n",
    "#(and the train-test split) will produce the same results each run.\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(transformed_X,\n",
    "                                                   Y,\n",
    "                                                   test_size=0.2)\n",
    "\n",
    "model= RandomForestRegressor()\n",
    "model.fit(X_train,Y_train)\n",
    "model.score(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ec1a3f-9b23-4e45-be2f-84e27f3174a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(car_sales_filled), len(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b5a0b-dcbe-4fa4-8d93-50988fb5f1ce",
   "metadata": {},
   "source": [
    "## Choosing the right model for your problem.\n",
    "\n",
    "Somethings to note:\n",
    "\n",
    "* SKlearn refers to machine learning models, algorithms as estimators.\n",
    "* Classification problem - predicting a catagory (heart disease or not)\n",
    "* sometimes you will see clf(short for classifier) used as a classification estimator\n",
    "* Regrassion problem - predicting a number(selling price of a car)\n",
    "* Clustering: Groups similar items into clusters to uncover inherent structures; for example, customer  segmentation in marketing to target customers with similar buying habits.\n",
    "* Dimensionality Reduction: Reduces the number of variables in high-dimensional data, improving visualization and efficiency; for example, reducing features in a dataset for facial recognition to improve model performance and reduce complexity.\n",
    "* Sklearn machine learning map:  https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8463067d-548a-49a7-bc00-36778c4158d4",
   "metadata": {},
   "source": [
    "### Picking a machine learning\n",
    "Let\"s use the California Housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6197b23c-1e35-4911-a25c-378bd8870cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get California Housing dataset\n",
    "# The code fetches the California Housing dataset from scikit-learn and stores it in the variable housing,\n",
    "# providing data for machine learning tasks related to housing prices.\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a3afdb-fb0e-4a33-a792-1237257ce0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets change it into a dataframe\n",
    "housing_df = pd.DataFrame(housing[\"data\"], columns=housing[\"feature_names\"])\n",
    "housing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d45ea99-21e4-4bb8-a448-6f33fbcc6e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets add a target column \n",
    "housing_df[\"MedHouseVal\"] = housing[\"target\"]\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e0f44-1656-45b1-a8a5-27476de182c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import algorithm using map\n",
    "# we will need to experiment to find the best algorithm\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create the data\n",
    "X = housing_df.drop(\"MedHouseVal\", axis= 1)\n",
    "Y = housing_df[\"MedHouseVal\"]  # median house price in 100,000s\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, Y_train,Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "\n",
    "# Instantiate and fit the model(on the training set)\n",
    "model = Ridge()\n",
    "model.fit(X_train,Y_train)\n",
    "\n",
    "# Check the score of the model(on teh test set)\n",
    "model.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a74e9b-3836-4f76-a6c2-4dee3a2da749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import algorithm using map\n",
    "# we will need to experiment to find the best algorithm\n",
    "# lets try another algoritm Lasso\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create the data\n",
    "X = housing_df.drop(\"MedHouseVal\", axis= 1)\n",
    "Y = housing_df[\"MedHouseVal\"]  # median house price in 100,000s\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, Y_train,Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "\n",
    "# Instantiate and fit the model(on the training set)\n",
    "model = Lasso()\n",
    "model.fit(X_train,Y_train)\n",
    "\n",
    "# Check the score of the model(on teh test set)\n",
    "model.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c4efdf-b86a-4ddb-8ae0-97c3666ac00a",
   "metadata": {},
   "source": [
    "When the algorithem we selected does not work or the score is not what we are looking for.\n",
    "We can always try a different model\n",
    "\n",
    "We can try ensembel model \n",
    "\n",
    "In scikit-learn, an ensemble model combines multiple machine learning models, like decision trees or classifiers, to improve prediction accuracy, stability, and robustness compared to using a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be4cc0a-3144-41ad-858a-ea23a401d71e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Import the  RandRandomForestRegressor model from the ensemble module\n",
    "from sklearn.ensemble import  RandomForestRegressor\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create the data \n",
    "X = housing_df.drop(\"MedHouseVal\", axis=1)\n",
    "Y = housing_df[\"MedHouseVal\"]\n",
    "\n",
    "# Split into train and test\n",
    "\n",
    "X_train, X_test , Y_train, Y_test = train_test_split(X,Y, test_size =0.2)\n",
    "\n",
    "# Create random forest model.\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# check the score of the model (on the test set)\n",
    "model.score(X_test, Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa3fc88-6ae6-4a71-ae49-fbb71b010f0b",
   "metadata": {},
   "source": [
    "## Picking a machine learnnig model for a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df07a1a4-e741-439a-8047-9810490488bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease = pd.read_csv(\"heart-disease.csv\")\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e050119-2db6-4489-a475-4e44d70f8a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(heart_disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316191e3-f762-464c-88ae-1509ecfe11ca",
   "metadata": {},
   "source": [
    "Going through the map we find that we can use LinearSVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c093d-c297-4a46-a375-1203f95e07d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LinearSVC estimator class\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# make the data\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "Y = heart_disease[\"target\"]\n",
    "\n",
    "# split the data\n",
    "\n",
    "X_train, X_test,Y_train, Y_test  = train_test_split(X,Y, test_size = 0.2)\n",
    "\n",
    "# Instantiate LinearSVC \n",
    "\n",
    "Clf = LinearSVC(max_iter=100000)\n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "# evaluate the LinearSVC\n",
    "\n",
    "clf.score(X_test,Y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6be1e-5c9e-47d9-95ba-c303b825a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets compare it with another classifier\n",
    "# Import the RandomForestClassifier estimator class\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make the data\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "Y = heart_disease[\"target\"]\n",
    "\n",
    "# split the data\n",
    "\n",
    "X_train, X_test,Y_train, Y_test  = train_test_split(X,Y, test_size = 0.2)\n",
    "\n",
    "# Instantiate Random Forest Classifier\n",
    "\n",
    "Clf = RandomForestClassifier()\n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "# evaluate the RandomForestClassifier\n",
    "clf.score(X_test,Y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b93475-26d1-4e91-8c1f-91d647d6d457",
   "metadata": {},
   "source": [
    "Note:\n",
    "1. If you have structured data, use Ensemble methods.\n",
    "2.  If you have unstructured data, use deep learning or transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0785c41-ecc1-46bb-aae1-907852c226ff",
   "metadata": {},
   "source": [
    "##  Fit the model on our data and use it to make pridictions\n",
    "\n",
    "Note Diferent names for X and Y:\n",
    "* 'X' = features, feature variables data\n",
    "* 'Y'= labels, targets, target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac89dd9-52c3-4f35-9a47-aeb6ca4aca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the RandomForestClassifer estimator class\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make the data\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "Y = heart_disease[\"target\"]\n",
    "\n",
    "# split the data\n",
    "\n",
    "X_train, X_test,Y_train, Y_test  = train_test_split(X,Y, test_size = 0.2)\n",
    "\n",
    "# Fit the model to the data\n",
    "\n",
    "Clf = RandomForestClassifier()\n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "# evaluate the RandomForestClassifier\n",
    "\n",
    "clf.score(X_test,Y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a612e3db-f8c6-464a-a090-b6dc7b4ceab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbed310d-ea89-4f0f-b00c-b8aec0d4d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0383e13-329e-4b0c-93d3-d4d0c7e06810",
   "metadata": {},
   "source": [
    "###  Make predictions using a machine learning model.\n",
    "\n",
    "2 ways to make predictions:\n",
    "\n",
    "1.predict()\n",
    "2.predict_proba()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71236c87-793f-423f-9c1d-eae2dd4fdd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a trained model to make predictions\n",
    "clf.predict(np.array([1,3,4,5,6]))  # this does not work\n",
    "\n",
    "# \"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\n",
    "#    \"Reshape your data either using array.reshape(-1, 1) if \"\n",
    "#          \"your data has a single feature or array.reshape(1, -1) \"\n",
    "#           \"if it contains a single sample.\".format(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e721ffb-5746-43c1-9c9a-9823fb5df7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to use the same format , it has to be a 2d array.\n",
    "# we can use the X_test as our mode has not seen this data and is a same format.\n",
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3205b28-0d23-45cf-aa61-cec5ae870a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the reults from our data we had split earlier\n",
    "\n",
    "np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e385c2-2cbf-4bd6-abfc-28e28185789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions to the truth labels to evaluate the model.\n",
    "Y_preds = clf.predict(X_test)\n",
    "np.mean(Y_preds == Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd400ca5-b7ca-4048-8e38-1bb75c0bfc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above result is thte same as the score we got earlier\n",
    "clf.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b746a1-1434-4441-82d6-1e4ae288e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to score the model.\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test,Y_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265e9859-7b5f-4a69-b3e6-6a39e639714a",
   "metadata": {},
   "source": [
    "### Make prediction with predict_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fc2af9-afb9-4e10-adc6-aad64288375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the classifier (clf) to predict probabilities of classification labels for the first 5 samples in X_test\n",
    "# The clf.predict_proba() function returns the predicted probabilities for each class\n",
    "predicted_probabilities = clf.predict_proba(X_test[:5])\n",
    "\n",
    "# Display the predicted probabilities\n",
    "predicted_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e756306d-1e11-4624-8f89-1887cfa4eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets predict it the other way\n",
    "clf.predict(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a746492-37e0-4acf-b859-b3e2cb3065a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc4d90-af6e-4d31-a69d-65a90a4d8056",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef299c0-9496-42d5-a514-1b70e6fcaec6",
   "metadata": {},
   "source": [
    "### predict() can also be used for regression models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e97a665-2138-479d-8365-5746151fa56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee5ab7-60a7-4f5e-adfc-98f51758ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# create the data\n",
    "\n",
    "X = housing_df.drop(\"MedHouseVal\", axis=1)\n",
    "Y = housing_df[\"MedHouseVal\"]\n",
    "\n",
    "\n",
    "# Split into training ans test sets.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "\n",
    "# Create the model instance\n",
    "# Create a Random Forest Regressor model with 10 decision trees\n",
    "# The RandomForestRegressor is an ensemble learning method that builds a forest of decision trees\n",
    "# n_estimators is a hyperparameter that determines the number of decision trees in the forest\n",
    "# Setting n_estimators=10 means the model will use 10 decision trees for making predictions\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100) # default n_estimatrors is = 100\n",
    "\n",
    "\n",
    "# Train (fit) the RandomForestRegressor model on the training data\n",
    "# X_train: Input features of the training data\n",
    "# Y_train: Target values corresponding to the training data\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Use the trained RandomForestRegressor model to make predictions on the test data\n",
    "# X_test: Input features of the test data\n",
    "# Y_preds: Predicted target values based on the input features\n",
    "\n",
    "Y_preds = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd7194-5288-436f-adcb-a7fadc9e23f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 predicted target values from the RandomForestRegressor model\n",
    "# Y_preds[:10]: Subset of predicted target values for the first 10 samples in the test data\n",
    "Y_preds[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b411b7c-d4b9-44fe-bfe8-a0e24432dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the true target values of the first 10 samples in the test data to a NumPy array\n",
    "# np.array(Y_test[:10]): Creating a NumPy array from the true target values for comparison\n",
    "np.array(Y_test[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b40f62-1486-40f6-b76b-b41a938a11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e20dc-9000-41ac-b27a-2a3d19b80792",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be43193-a357-487a-b9a2-fcd2fb7fabf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the mean_absolute_error function to calculate the mean absolute error (MAE)\n",
    "# MAE is a metric that measures the average absolute difference between the true and predicted values\n",
    "# It provides a straightforward way to understand the average magnitude of errors in the predictions\n",
    "# Lower MAE indicates better model performance, with 0 being a perfect match\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(Y_test, Y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78834f12-4d47-4b4b-8a56-a36203d43e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df[\"MedHouseVal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46af7879-f66b-429f-853d-eb1a9e9dd621",
   "metadata": {},
   "outputs": [],
   "source": [
    "What_we_are_going_to_cover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89adf144-171b-4403-a4ec-82c73b61e3ee",
   "metadata": {},
   "source": [
    "## Evaluating a machine learning model \n",
    "\n",
    "Three ways to evaluate Scikit-learn models:\n",
    "\n",
    "     1. Estimator's built-in `score()`  method\n",
    "     2. The `scoring` parameter\n",
    "     3. Problem-specific metric functions.\n",
    "\n",
    "\n",
    "You can find more information here: https://scikit-learn.org/stable/modules/model_evaluation.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d589cf-8495-48ab-ae15-3ebd504f6bfb",
   "metadata": {},
   "source": [
    "### Evaluating a model with the `score` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29065f-2c65-4aac-a83a-2fe75b3d2a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Create X & Y\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "Y = heart_disease[\"target\"]\n",
    "\n",
    "\n",
    "# Create train/test\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2)\n",
    "\n",
    "# Instantiate Random Forest Classifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "\n",
    "# Fit classifer to training data\n",
    "clf.fit (X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9fac13-87c3-4700-82f3-e882c8180638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The highest value for the .score() method is 1.0 the lowest is 0.0\n",
    "# The model is geting a 100% score as it has seen all the training data.\n",
    "clf.score(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a619a-934d-4284-8ac9-ab066114a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be the score on the unseed data \n",
    "clf.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2bd5bb-dd8e-4a2c-aff4-db77cf18d0eb",
   "metadata": {},
   "source": [
    "Lets try the `score()` method  on a regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c85835-433e-4109-83b8-c58e0e97e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# create the data\n",
    "\n",
    "X = housing_df.drop(\"MedHouseVal\", axis=1)\n",
    "Y = housing_df[\"MedHouseVal\"]\n",
    "\n",
    "\n",
    "# Split into training ans test sets.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "\n",
    "# Create the model instance\n",
    "# Create a Random Forest Regressor model with 10 decision trees\n",
    "# The RandomForestRegressor is an ensemble learning method that builds a forest of decision trees\n",
    "# n_estimators is a hyperparameter that determines the number of decision trees in the forest\n",
    "# Setting n_estimators=10 means the model will use 10 decision trees for making predictions\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100) # default n_estimatrors is = 100\n",
    "\n",
    "\n",
    "# Train (fit) the RandomForestRegressor model on the training data\n",
    "# X_train: Input features of the training data\n",
    "# Y_train: Target values corresponding to the training data\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e9b17-1272-437d-bd95-4709908ebd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the regression model 'model' on the test dataset.\n",
    "# 'X_test' contains the test features, and 'Y_test' contains the true labels.\n",
    "# For regression algorithms, the default metric used by 'score()' is R-squared (r_squared).\n",
    "# R-squared values range from 0.0 (worst) to 1.0 (best), indicating the proportion of \n",
    "# variance for the dependent variable that's explained by the independent variables in the model.\n",
    "model.score(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6691671b-0ab4-43c0-a165-0b8e82813488",
   "metadata": {},
   "source": [
    "## Evaluating a model using the `score` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af6219-8d25-4a44-8bf4-44b03521f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for machine learning\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Set a random seed for reproducibility of results\n",
    "np.random.seed(42)\n",
    "\n",
    "# Prepare the feature matrix (X) by dropping the target variable column\n",
    "X = housing_df.drop(\"MedHouseVal\", axis=1)  # 'MedHouseVal' is the target variable\n",
    "\n",
    "# Prepare the target vector (Y) by selecting only the target column\n",
    "Y = housing_df[\"MedHouseVal\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# 80% of the data is used for training and 20% for testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# Initialize the RandomForestRegressor model\n",
    "# Setting n_estimators to 100, specifying the number of trees in the forest\n",
    "clf = RandomForestRegressor(n_estimators=100)  # Default n_estimators is 100\n",
    "\n",
    "# Fit the RandomForestRegressor model on the training data\n",
    "# This step involves the model learning the relationship between features and the target variable\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# After this, clf is now a trained RandomForestRegressor model\n",
    "# It can be used to make predictions and evaluate its performance on unseen data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2a4d0-7476-4a7f-ba66-183bb7b75bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of the classifier 'clf' on the test dataset\n",
    "# 'X_test' contains the test features, and 'Y_test' contains the true labels.\n",
    "# The 'score' method compares the model's predictions with the true labels \n",
    "# to calculate the accuracy (for classification) or R^2 score (for regression).\n",
    "clf.score(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443e876d-bace-4d64-85fa-1c7e1f08639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation on the classifier 'clf' using the dataset 'X' and labels 'Y'.\n",
    "# The 'cross_val_score' function splits 'X' and 'Y' into multiple subsets (folds), \n",
    "# and for each fold, it trains the classifier 'clf' on the fold's training set \n",
    "# and evaluates its performance on the fold's test set.\n",
    "# Here, 'cv=5' specifies that the data should be split into 5 folds, \n",
    "# providing a balance between training and testing each subset.\n",
    "# This approach helps in assessing the model's effectiveness and \n",
    "# generalization capability over different data samples.\n",
    "cross_val_score(clf, X, Y, cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5db724c-4324-4b51-babd-979042dc03f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Set a random seed for reproducibility of the experiment\n",
    "np.random.seed(420)\n",
    "\n",
    "# Evaluating the RandomForestRegressor model on a single training/test split\n",
    "# This gives us an idea of how the model performs on this particular partition of the data\n",
    "clf_single_score = clf.score(X_test, Y_test)\n",
    "\n",
    "# Evaluating the model using 5-fold cross-validation\n",
    "# This method splits the entire dataset into 5 parts, trains the model 5 times using 4 parts as training data\n",
    "# and the remaining part as testing data each time, and calculates the score for each iteration.\n",
    "# Taking the mean of these scores gives us a more robust estimate of the model's performance.\n",
    "clf_cross_val_score = np.mean(cross_val_score(clf, X, Y, cv=5))\n",
    "\n",
    "# Compare the two scores to understand how the model's performance might vary\n",
    "# between a single split and across different folds of cross-validation.\n",
    "# A significant difference might indicate the model's sensitivity to the data partitioning.\n",
    "(clf_single_score, clf_cross_val_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a6465e-5b3a-4d59-afe1-d10651e5215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring parameter set to None by default\n",
    "# Default scoring parameter of classifier = mean accuracy\n",
    "\n",
    "# Perform cross-validation on the classifier 'clf' using the dataset 'X' and labels 'Y'.\n",
    "# The 'cross_val_score' function with 'scoring=None' defaults to the classifier's \n",
    "# standard scoring method, which for most classifiers is mean accuracy.\n",
    "# 'cv=5' specifies that the data is split into 5 subsets (folds).\n",
    "# In each of the 5 iterations, the classifier is trained on 4 folds and tested on the remaining fold.\n",
    "# This provides a robust evaluation of the classifier's performance across different subsets of the data.\n",
    "cross_val_score(clf, X, Y, cv=5, scoring=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f89c497-ad3d-4142-86c6-18b9f82373f0",
   "metadata": {},
   "source": [
    "### Classification model evaluation metrics\n",
    "\n",
    "1. Accuracy \n",
    "2. Area under ROC curve\n",
    "3. Confusion matrix\n",
    "4. Classification report\n",
    "\n",
    "\n",
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbbd32d-8586-4494-b615-c83526d3b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions and models from scikit-learn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Ensuring reproducibility of results by setting a random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# with 'target' as the column indicating the presence or absence of heart disease.\n",
    "# Preparing the feature matrix (X) and target vector (Y)\n",
    "X = heart_disease.drop(\"target\", axis=1)  # Features: all columns except 'target'\n",
    "Y = heart_disease[\"target\"]  # Target: the 'target' column\n",
    "\n",
    "# Instantiate a RandomForestClassifier with 50 trees\n",
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "# Perform 5-fold cross-validation to evaluate the model\n",
    "# The function cross_val_score() automatically splits the data into folds,\n",
    "# trains the model on the training folds, and tests it on the testing fold for each split.\n",
    "# cv=5 specifies that the data should be split into 5 parts for cross-validation.\n",
    "cross_val_scores = cross_val_score(clf, X, Y, cv=5)\n",
    "\n",
    "# cross_val_scores now contains the accuracy scores of the model for each fold.\n",
    "# You can further analyze these scores (e.g., calculate the mean) to estimate the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f74f6f-0676-4742-a582-32c1de1cb034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the cross-validation scores\n",
    "mean_cross_val_score = np.mean(cross_val_scores)\n",
    "\n",
    "# Print the mean cross-validation score\n",
    "print(\"Mean cross-validation score:\", mean_cross_val_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6989276-818f-4085-b580-957b90acf7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Heart Disease Classifier Cross-validated Accuracy: {np.mean(cross_val_scores) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9914e7-c622-4a03-8e8f-78ff7ee38889",
   "metadata": {},
   "source": [
    "**Area under the reciever operating characteristic curve(AUC/ROC)**\n",
    "\n",
    "\n",
    "* Area under curve (AUC)\n",
    "*ROC curve\n",
    "\n",
    "ROC curves are a comparison of a model's true postive rate (tpr) versus a model false\n",
    "positive rate (fpr).\n",
    "\n",
    "* True positive = model predicts 1 when truth is 1\n",
    "* False positive = model predicts 1 when truth is 0 \n",
    "* True negative = model predicts 0 when truth is 0\n",
    "* False negative = model predicts 0 when truth is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505e2ca-4307-4109-8aa1-35b75d4c3ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets.\n",
    "# 'train_test_split' randomly divides the data into two parts:\n",
    "# 1. Training set (X_train and Y_train) used for the model to learn from.\n",
    "# 2. Testing set (X_test and Y_test) used to evaluate the model's performance.\n",
    "# The 'test_size=0.2' argument specifies that 20% of the data should be held out for testing,\n",
    "# meaning the remaining 80% is used for training the model.\n",
    "# This function ensures that your model is tested on unseen data, simulating real-world predictions.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# After this step, you have four subsets of data:\n",
    "# X_train: The feature variables used for training.\n",
    "# Y_train: The target variable corresponding to X_train, used for training.\n",
    "# X_test: The feature variables set aside for testing the model.\n",
    "# Y_test: The target variable corresponding to X_test, used to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973059a6-ae31-4b52-8a77-1f7c312c6b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "# This step trains the model using the features (X_train) and the target (Y_train).\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions with probabilities on the test set\n",
    "# The predict_proba method returns the probability estimates for all classes,\n",
    "# which is useful for evaluating the classifier's performance through metrics like ROC.\n",
    "Y_probs = clf.predict_proba(X_test)\n",
    "\n",
    "# Display the first 10 prediction probabilities and the total number of predictions\n",
    "Y_probs[:10], len(Y_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b0b93-b4e0-434e-80ba-ac587f698b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract positive class probabilities\n",
    "# The [:,1] indexing selects the probabilities of the positive class (usually '1') for all instances.\n",
    "# This is typically required for metrics like ROC AUC, where the focus is on the performance concerning the positive class.\n",
    "Y_probs_positive = Y_probs[:, 1]\n",
    "\n",
    "# Display the first 10 positive class probabilities\n",
    "Y_probs_positive[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f695c987-8910-4db4-ab0e-dd7cbc7a257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Calculate False Positive Rate (fpr), True Positive Rate (tpr), and thresholds\n",
    "# The roc_curve function computes the Receiver Operating Characteristic (ROC) curve,\n",
    "# which illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n",
    "# It's a plot of the True Positive Rate (tpr) against the False Positive Rate (fpr) at various threshold settings.\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, Y_probs_positive)\n",
    "\n",
    "# Display the False Positive Rates (fpr)\n",
    "# The fpr array contains the false positive rates for each threshold used to compute the ROC curve.\n",
    "# A false positive rate indicates how often the model incorrectly classifies a negative observation as positive.\n",
    "fpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7152975-fdba-4da2-ae76-6f1f6dc4ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "    \"\"\"\n",
    "    Plots the Receiver Operating Characteristic (ROC) curve.\n",
    "    \n",
    "    The ROC curve visualizes the performance of a classification model by plotting\n",
    "    the true positive rate (TPR) against the false positive rate (FPR) at various threshold levels.\n",
    "    \n",
    "    Parameters:\n",
    "    - fpr: Array of false positive rates.\n",
    "    - tpr: Array of true positive rates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plot the ROC curve using the FPR and TPR values\n",
    "    plt.plot(fpr, tpr, color=\"orange\", label=\"ROC\")\n",
    "    \n",
    "    # Plot a diagonal dashed line representing a no-skill classifier (baseline)\n",
    "    plt.plot([0, 1], [0, 1], color=\"darkblue\", linestyle=\"--\", label=\"No Skill\")\n",
    "    \n",
    "    # Add labels and title for clarity\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    \n",
    "    # Display the legend to differentiate between the ROC curve and the baseline\n",
    "    plt.legend()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage of the function (assuming fpr and tpr are defined from a model's predictions)\n",
    "plot_roc_curve(fpr, tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f541fb7-a9a6-45ad-9c92-0880ad9ecc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Calculate the ROC AUC Score\n",
    "# The roc_auc_score function computes the area under the ROC curve,\n",
    "# which represents the model's ability to discriminate between positive and negative classes.\n",
    "# A higher score indicates better performance, with 1.0 representing a perfect model.\n",
    "roc_auc = roc_auc_score(Y_test, Y_probs_positive)\n",
    "\n",
    "# Print the ROC AUC score to evaluate the model\n",
    "print(f\"ROC AUC Score: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb07456-1b68-4a6d-92a1-7975a6e63dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot perfect ROC cueve and AUC score\n",
    "fpr, tpr, thresholds =roc_curve(Y_test, Y_test)\n",
    "plot_roc_curve(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3750cb53-5a14-4644-abfd-6c3651fc6749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfect AUC score is \n",
    "roc_auc_score(Y_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f861beb3-58c9-4dd5-97ef-4361d51cd949",
   "metadata": {},
   "source": [
    "**Confusion Matrix**\n",
    "\n",
    "A confusion matrix serves as a rapid tool for comparing the predicted labels of a model with the actual labels it was expected to predict. Essentially, it provides insights into the areas where the model encounters confusion, offering a clear visualization of its performance across different classes or categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6075299-68f0-4729-8aef-769b5e0399be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More info on confusion matrices:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "# Importing the confusion_matrix function from sklearn.metrics module\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Making predictions using the trained classifier (clf) on the test data (X_test)\n",
    "Y_preds = clf.predict(X_test)\n",
    "\n",
    "# Generating the confusion matrix to evaluate the performance of the classifier\n",
    "# The confusion matrix compares the actual labels (Y_test) with the predicted labels (Y_preds)\n",
    "# It provides insights into the classifier's performance in terms of true positives, false positives,\n",
    "# true negatives, and false negatives across different classes.\n",
    "confusion_matrix(Y_test, Y_preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d225f-c64a-46ac-8b88-633928ce4dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing confusion matrix with pd.crosstab()\n",
    "\n",
    "# Importing pandas library for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Creating a cross-tabulation (crosstab) of actual vs. predicted labels\n",
    "# The crosstab function computes a simple cross-tabulation of two (or more) factors.\n",
    "# It computes a frequency table of the factors, which in this case are the actual labels (Y_test) and predicted labels (Y_preds).\n",
    "# The rownames and colnames parameters are used to specify the names for the rows and columns of the resulting DataFrame,\n",
    "# which correspond to the actual and predicted labels, respectively.\n",
    "confusion_matrix_df = pd.crosstab(Y_test,\n",
    "                                   Y_preds,\n",
    "                                   rownames=[\"Actual Labels\"],\n",
    "                                   colnames=[\"Predicted Labels\"]\n",
    "                                  )\n",
    "\n",
    "# Displaying the cross-tabulated confusion matrix\n",
    "confusion_matrix_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47f13e-30f7-4c18-bf1e-aa1a4657505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "22 + 7 + 8 + 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68a32c-5f32-474a-8ccb-1e1fd6bea6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7deab4e-2966-42ce-b619-8e5b68aacb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing a Conda package into the current environment from a Jupyter Notebook\n",
    "import sys\n",
    "\n",
    "# The following command installs the 'seaborn' package into the current Conda environment.\n",
    "# '--yes' flag confirms the installation without user confirmation.\n",
    "# '--prefix {sys.prefix}' specifies the path to the current Conda environment.\n",
    "!conda install --yes --prefix {sys.prefix} seaborn \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13fa444-6fa8-4c7f-ba61-b6daf751059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the confusion matrix more visual using Seaborn's heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the font scale for better readability\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_mat = confusion_matrix(Y_test, Y_preds)\n",
    "\n",
    "# Plot the confusion matrix using Seaborn's heatmap\n",
    "plt.figure(figsize=(10, 8))  # Adjust the figure size if needed\n",
    "sns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"Actual Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a241f41-45c2-4380-9047-d4b94cfdcf84",
   "metadata": {},
   "source": [
    "### Creating a confussion matrix using scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef50f3c-93c4-488e-ae62-f50376838298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99da6f-5d46-40ef-abef-97f23c23bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b025f62-8c14-4917-8004-3bd5de202b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the ConfusionMatrixDisplay class from sklearn.metrics module\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Generating predictions using the classifier (clf) on the features (X)\n",
    "Y_preds = clf.predict(X)\n",
    "\n",
    "# Creating a ConfusionMatrixDisplay object from the predictions and true labels\n",
    "# The ConfusionMatrixDisplay class provides a way to visualize confusion matrices.\n",
    "# It requires predicted labels (Y_preds) and true labels (Y) as input.\n",
    "# Since we already have the predicted labels (Y_preds), we can directly use them.\n",
    "# Note: Make sure the classifier (clf) has been trained and fitted before making predictions.\n",
    "confusion_matrix_display = ConfusionMatrixDisplay.from_predictions(y_true=Y, y_pred=Y_preds)\n",
    "\n",
    "# Displaying the confusion matrix\n",
    "confusion_matrix_display.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e1deba-31dd-4c7a-b7c7-3df5f8edaac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Perform cross-validation predictions\n",
    "y_pred = cross_val_predict(clf, X, Y, cv=5)\n",
    "\n",
    "# Import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Create ConfusionMatrixDisplay object\n",
    "confusion_matrix_display = ConfusionMatrixDisplay.from_predictions(y_true=Y, y_pred=y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "confusion_matrix_display.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd48777-0a52-4af5-a957-5d34c37434bc",
   "metadata": {},
   "source": [
    "**Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c2852-b9fe-468d-9292-9751800db107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of Y_test and Y_preds\n",
    "print(\"Shape of Y_test:\", Y_test.shape)\n",
    "print(\"Shape of Y_preds:\", Y_preds.shape)\n",
    "\n",
    "# Check the length of Y_test and Y_preds\n",
    "print(\"Length of Y_test:\", len(Y_test))\n",
    "print(\"Length of Y_preds:\", len(Y_preds))\n",
    "\n",
    "# If the lengths don't match, print the first few elements to identify any discrepancies\n",
    "if len(Y_test) != len(Y_preds):\n",
    "    print(\"First 10 elements of Y_test:\", Y_test[:10])\n",
    "    print(\"First 10 elements of Y_preds:\", Y_preds[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f238e-97bf-40d0-a879-74a37cb1418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model using the training data (X_train and Y_train)\n",
    "# For example, if 'clf' is a classifier model, fit it to the training data:\n",
    "# clf.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the test data (X_test) using the trained model\n",
    "# Store the predicted labels in Y_preds\n",
    "Y_preds = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model by generating a classification report\n",
    "# The classification report provides precision, recall, F1-score, and support for each class\n",
    "# Compare the true labels (Y_test) with the predicted labels (Y_preds)\n",
    "print(classification_report(Y_test, Y_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e06ed-2cf1-4e18-a18c-b3cbf7c1c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where precision and recall become valuable\n",
    "\n",
    "disease_true = np.zeros(10000)\n",
    "disease_true[0] = 1 # only one positive case\n",
    "\n",
    "disease_preds = np.zeros(10000) # model predicts every case as 0\n",
    "\n",
    "pd.DataFrame(classification_report(disease_true,disease_preds,output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a7d351-2777-4770-8e26-380100e1eb3e",
   "metadata": {},
   "source": [
    "To summerize clasiification metrics:\n",
    "\n",
    "* **Accuracy** is a good measure to start with if all class are balanced (e.g. same amount of samples which are labelledd with 0 and 1)\n",
    "*  **Percision** and **Recall** become more important when classed are imbalanced.\n",
    "*  If false positives predictions are worse than false negatives, aim for higher percision.\n",
    "*  If false negatives predictions are worse than false postive, aim for higher call.\n",
    "*   **F1-score** is a combination of percision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f848954a-c279-44d7-9752-59740f0a0116",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9299cb39-33bd-4508-a172-4c9c04e4622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = housing_df.drop(\"MedHouseVal\", axis=1)\n",
    "Y = housing_df[\"MedHouseVal\"]\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "\n",
    "model= RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659c706-ce61-458d-aa00-8e27ee5b1e94",
   "metadata": {},
   "source": [
    "### Regression model evaluation metrics\n",
    "Model evaluation metrics documentation - https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n",
    "\n",
    "Let's go through a few of them:\n",
    "\n",
    "*  RÂ² score, the coefficient of determination \n",
    "*  Mean absolute error (MAE)\n",
    "*  Mean squared error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b2706-7781-4b71-88a0-a64d41ec9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# create the data\n",
    "\n",
    "X = housing_df.drop(\"MedHouseVal\", axis=1)\n",
    "Y = housing_df[\"MedHouseVal\"]\n",
    "\n",
    "\n",
    "# Split into training ans test sets.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "\n",
    "# Create the model instance\n",
    "# Create a Random Forest Regressor model with 10 decision trees\n",
    "# The RandomForestRegressor is an ensemble learning method that builds a forest of decision trees\n",
    "# n_estimators is a hyperparameter that determines the number of decision trees in the forest\n",
    "# Setting n_estimators=10 means the model will use 10 decision trees for making predictions\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100) # default n_estimatrors is = 100\n",
    "\n",
    "\n",
    "# Train (fit) the RandomForestRegressor model on the training data\n",
    "# X_train: Input features of the training data\n",
    "# Y_train: Target values corresponding to the training data\n",
    "\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd096bd-fbdf-482b-a63b-85fb8582625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07635b-d164-48e2-af67-7bc278fc5834",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a4a36-34b3-4dca-a22b-9bf8a5407081",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a201a5-9b76-4fa9-853e-fd414bfe7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the target variable in the test set (Y_test).\n",
    "Y_test_mean = Y_test.mean()  # Calculate the mean of Y_test\n",
    "\n",
    "# The mean of Y_test can provide insights into the distribution of the target variable.\n",
    "# For binary classification problems, where the target variable represents a binary outcome (e.g., presence or absence of a condition),\n",
    "# the mean can indicate the prevalence of the positive class (e.g., the proportion of positive instances in the test set).\n",
    "# For example, in a binary classification problem where 1 represents the positive class and 0 represents the negative class,\n",
    "# the mean of Y_test can give the proportion of positive instances in the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee30834-5ecd-4ea6-9669-9d0739c4b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the r2_score function from the sklearn.metrics module\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Creating an array filled with the mean of Y_test\n",
    "Y_test_mean = np.full(len(Y_test), Y_test.mean())\n",
    "\n",
    "# The r2_score function calculates the coefficient of determination (R^2) regression score.\n",
    "# It measures the proportion of the variance in the dependent variable (Y_test) that is predictable from the independent variable (Y_preds).\n",
    "# R^2 score ranges from 0 to 1, where 1 indicates a perfect fit.\n",
    "# Comparing the predictions (Y_preds) with the mean of Y_test can help evaluate the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be618deb-8d38-405c-b300-5ed11ad78c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_mean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18f4d65-231d-4c01-ad19-fee07c77f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the R-squared score:\n",
    "# - `y_true=Y_test` specifies the actual values from the test dataset.\n",
    "# - `y_pred=Y_test_mean` uses the mean of Y_test values as the prediction.\n",
    "# This score evaluates how close the mean predictions are to the actual values.\n",
    "r2_score(y_true=Y_test, y_pred=Y_test_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe51c607-30ba-4a70-9bad-9fd554b75e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the R-squared score where both the true values and predicted values are `Y_test`.\n",
    "# Since `y_true` and `y_pred` are the same, the R-squared score will be 1.0,\n",
    "# indicating a perfect fit. This is expected in this scenario because the \"predictions\" are exactly the same as the actual values.\n",
    "r2_score(y_true=Y_test, y_pred=Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe5dff-97a8-46ff-aee6-8700046ab75e",
   "metadata": {},
   "source": [
    "**Mean absolute error (MAE)**\n",
    "\n",
    "MEA is the average of the absolute difference beteween predictions and actual values.\n",
    "\n",
    "It gives you an idea of how wrong your models predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c0047-52d7-44c8-a2b5-9c813748ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Mean Absolute Error (MAE) between the actual values (Y_test) and the predicted values (Y_preds).\n",
    "# 1. `Y_preds = model.predict(X_test)`: Predict the Y values using the model and test features (X_test).\n",
    "# 2. `mean_absolute_error(Y_test, Y_preds)`: Calculate the MAE, which quantifies the average magnitude of errors between the predicted and actual values, without considering their direction.\n",
    "# The result is stored in the variable `mae`.\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "Y_preds = model.predict(X_test)\n",
    "mae = mean_absolute_error(Y_test, Y_preds)  \n",
    "print(mae)  # Assuming you want to print the MAE value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4af940-ddcd-4184-b02d-a64d45452044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with actual and predicted values:\n",
    "# 1. `data`: a dictionary with keys 'actual values' and 'Predicted values', corresponding to Y_test and Y_preds.\n",
    "# 2. Calculate the differences between 'Predicted values' and 'actual values' for each row and store in a new column 'differences'.\n",
    "# 3. Display the first 10 rows of the DataFrame to inspect the actual values, predicted values, and their differences.\n",
    "\n",
    "df = pd.DataFrame(data={\"actual values\": Y_test,\n",
    "                        \"Predicted values\": Y_preds})\n",
    "df[\"differences\"] = df[\"Predicted values\"] - df[\"actual values\"]  # Ensure column names match when performing operations.\n",
    "df.head(10)  # Display the first 10 rows of the DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86cc14-40e2-4065-b5a9-e487c9d42459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Mean Absolute Error (MAE) manually:\n",
    "# 1. Compute the absolute value of differences between predicted and actual values.\n",
    "# 2. Take the mean of these absolute differences.\n",
    "# This operation provides the average magnitude of errors between the predictions and actual values, \n",
    "# serving as a measure of model accuracy.\n",
    "\n",
    "mean_absolute_error = np.abs(df[\"differences\"]).mean()\n",
    "print(mean_absolute_error)  # Print the calculated MAE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea035fe-8dcd-4e18-8df9-2339d5c2a718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
