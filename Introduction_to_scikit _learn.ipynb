{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f5621fa",
   "metadata": {},
   "source": [
    "# Introduction to Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a29d6c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " # This notebook demonstrates some of the most useful functions of the beautiful Scikit-Learn Library.\n",
    "\n",
    "What_we_are_going_to_cover = [\n",
    "\n",
    "\"0. An end -to-end Scikit learn workfolw.\",\n",
    "\"1. Getting the data ready.\",\n",
    "\"2. Choose the right estimator/algorith for our problems.\",\n",
    "\"3. Fit the model/algorithm and use it to make predictions on our data\",\n",
    "\"4. Evaluate a model.\",\n",
    "\"5. Improve a model.\",\n",
    "\"6. Save and Load a trained model.\",\n",
    "\"7. Putting it all together!\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c816c214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0. An end -to-end Scikit learn workfolw.',\n",
       " '1. Getting the data ready.',\n",
       " '2. Choose the right estimator/algorith for our problems.',\n",
       " '3. Fit the model/algorithm and use it to make predictions on our data',\n",
       " '4. Evaluate a model.',\n",
       " '5. Improve a model.',\n",
       " '6. Save and Load a trained model.',\n",
       " '7. Putting it all together!']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "What_we_are_going_to_cover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ad16e",
   "metadata": {},
   "source": [
    "## 0.An end-to-end Scikit-Learn Workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "532b7b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0     63    1   3       145   233    1        0      150      0      2.3   \n",
       "1     37    1   2       130   250    0        1      187      0      3.5   \n",
       "2     41    0   1       130   204    0        0      172      0      1.4   \n",
       "3     56    1   1       120   236    0        1      178      0      0.8   \n",
       "4     57    0   0       120   354    0        1      163      1      0.6   \n",
       "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       "298   57    0   0       140   241    0        1      123      1      0.2   \n",
       "299   45    1   3       110   264    0        1      132      0      1.2   \n",
       "300   68    1   0       144   193    1        1      141      0      3.4   \n",
       "301   57    1   0       130   131    0        1      115      1      1.2   \n",
       "302   57    0   1       130   236    0        0      174      0      0.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "0        0   0     1       1  \n",
       "1        0   0     2       1  \n",
       "2        2   0     2       1  \n",
       "3        2   0     2       1  \n",
       "4        2   0     2       1  \n",
       "..     ...  ..   ...     ...  \n",
       "298      1   0     3       0  \n",
       "299      1   0     3       0  \n",
       "300      1   2     3       0  \n",
       "301      1   1     3       0  \n",
       "302      1   1     2       0  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Get the data ready\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "heart_disease = pd.read_csv(\"heart-disease.csv\")\n",
    "heart_disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d576585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X (features matrix) \n",
    "\n",
    "# In summary, you are splitting your dataset into two parts:\n",
    "\n",
    "   # X, which contains all the independent variables (features) without the target column.\n",
    "   # Y, which contains only the target column.\n",
    "\n",
    "#This is a standard procedure in preparing data for supervised learning, where X is used to train the model, \n",
    "# and Y is what the model attempts to predict.\n",
    "# This separation is a common practice when preparing data for machine learning, \n",
    "# as it allows you to train your model to learn the relationship between the input features (X) \n",
    "# and the target output (Y).\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "\n",
    "# Create y (Labels)\n",
    "Y = heart_disease[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d872696f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Choose the right model and hyperparameters\n",
    "# clf is short for classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf  = RandomForestClassifier()\n",
    "\n",
    "# we'll keep the default hyperparameters\n",
    "\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7798fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 . Fit the model to the training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "# Importing the train_test_split function\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# This line imports the train_test_split function from the model_selection module of scikit-learn.\n",
    "# The train_test_split function is a utility that helps you easily split your dataset\n",
    "# into a training set and a testing set.\n",
    "\n",
    "# Splitting the Dataset into Training and Testing Sets\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "# This line is where the actual splitting of the data occurs.\n",
    "# train_test_split(X, Y, test_size=0.2) takes your features matrix X and your labels Y and splits \n",
    "# them into training and testing sets.\n",
    "# The test_size=0.2 argument specifies that 20% of the data should be set aside for testing.\n",
    "# This means that 80% of the data will be used for training the model.\n",
    "# The function returns four subsets:\n",
    "#    - X_train: The subset of your features used for training the model.\n",
    "#    - X_test: The subset of your features used for testing the model.\n",
    "#    - Y_train: The subset of your labels corresponding to X_train, used for training the model.\n",
    "#    - Y_test: The subset of your labels corresponding to X_test, used for evaluating the model's performance.\n",
    "\n",
    "# In simple terms, this process divides your dataset into two parts: \n",
    "# one part to train your machine learning model (the training set) and \n",
    "# another part to test its performance and see how well it generalizes to new, unseen data (the testing set).\n",
    "# This is a fundamental practice in machine learning to avoid overfitting, \n",
    "# where a model performs well on the training data but poorly on new data.\n",
    "# By evaluating the model on a separate testing set, \n",
    "# you get a better sense of its real-world performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4a2479e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, Y_train);\n",
    "\n",
    "# Fitting the model to the training data\n",
    "\n",
    "# clf.fit(X_train, Y_train)\n",
    "# This line is where the model 'learns' from the data.\n",
    "# The 'fit' method is used to train the model using the training data.\n",
    "# 'clf' is the instance of RandomForestClassifier we created earlier.\n",
    "\n",
    "# X_train: This is the training data (features), which the model uses to learn.\n",
    "# Y_train: These are the true labels for the training data.\n",
    "\n",
    "# During the fitting process, the RandomForestClassifier will look at the X_train and Y_train data,\n",
    "# and try to figure out the patterns or relationships between the features and the target label.\n",
    "# This process involves the RandomForestClassifier building multiple decision trees,\n",
    "# each looking at different aspects and combinations of the data,\n",
    "# and then combining their insights to make more accurate predictions.\n",
    "\n",
    "# Once the model is fitted, it can then be used to make predictions on new, unseen data.\n",
    "# The 'fit' method is one of the most fundamental and first steps in the model building process \n",
    "# in machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "96bfc365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0. 2. 3. 4.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Make a prediction\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m Y_label \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:823\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    803\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 823\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    826\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:865\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    863\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    868\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:599\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    598\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 599\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\utils\\validation.py:940\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 940\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    941\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    942\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    943\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    944\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    945\u001b[0m         )\n\u001b[0;32m    947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    949\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    950\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    951\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0. 2. 3. 4.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Make a prediction\n",
    "Y_label = clf.predict(np.array([0,2,3,4]))\n",
    "\n",
    "# this fails because the shape of the imput is not corect.\n",
    "# we are puting in a array.\n",
    "\n",
    "\n",
    "\n",
    "# Y_label = clf.predict(np.array([0,2,3,4]))\n",
    "# This line is attempting to use the trained model to make a prediction.\n",
    "# 'clf.predict()' is the method used to predict the label of new data using the trained model.\n",
    "\n",
    "# However, this line raises an error because the shape of the input data does not match the \n",
    "# shape the model expects.\n",
    "# The issue here is that we are passing a 1-dimensional array as input, \n",
    "# whereas the model expects the input to have the same number of features as the training data (X_train).\n",
    "\n",
    "# In this case, the model was trained on a certain number of features (columns in X_train),\n",
    "# but the input provided is a single array with only 4 values.\n",
    "# Each value in this array is being interpreted as a separate input example rather than a set of \n",
    "# features for a single example.\n",
    "\n",
    "# To fix this, the input data needs to be reshaped or reformatted to match the format of the training data.\n",
    "# If you're trying to predict for one example, the input should be a 2-dimensional array \n",
    "# with one row for the example and columns matching the number of features the model was trained on.\n",
    "\n",
    "# An example correction could be to reshape the array to have 1 row and the correct number of columns:\n",
    "# Y_label = clf.predict(np.array([[0, 2, 3, 4]]))\n",
    "# Note: The inner brackets create a 2D array with one row and four columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a4c37-ffa5-4e75-ab46-26e28f6ab9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the Error in Prediction Attempt\n",
    "\n",
    "# Error Message: \n",
    "# \"Expected 2D array, got 1D array instead: array=[0. 2. 3. 4.]. \n",
    "# Reshape your data either using array.reshape(-1, 1) if your data has a single feature \n",
    "# or array.reshape(1, -1) if it contains a single sample.\"\n",
    "\n",
    "# This error occurs because the input data provided to clf.predict() is not in the correct format.\n",
    "# The RandomForestClassifier model expects the input data to be a 2-dimensional array, \n",
    "# but the provided data is a 1-dimensional array.\n",
    "\n",
    "# In the context of the model, a 2D array represents a collection of samples, \n",
    "# where each sample has multiple features. \n",
    "# The model is trained on such an array, so it expects the same format for making predictions.\n",
    "\n",
    "# To fix this error, the input data needs to be reshaped into a 2D array. \n",
    "# Since the intention is to predict a single sample with multiple features, \n",
    "# you should reshape the array to have 1 row (representing 1 sample) and multiple columns \n",
    "# (representing features).\n",
    "\n",
    "# The correct way to reshape and make a prediction would be:\n",
    "# Y_label = clf.predict(np.array([0, 2, 3, 4]).reshape(1, -1))\n",
    "# Here, .reshape(1, -1) changes the shape of the array to have 1 row and as many columns as\n",
    "# necessary to accommodate the data.\n",
    "\n",
    "# This reshaping ensures that the data format matches what the model expects, \n",
    "# allowing the prediction method to work correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8ae5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it has to look like this for it to work\n",
    "# has to be a 2d array.\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_preds = clf.predict(X_test)\n",
    "Y_preds\n",
    "\n",
    "# Making Predictions on the Test Set\n",
    "\n",
    "# Y_preds = clf.predict(X_test)\n",
    "# This line uses the trained RandomForestClassifier (clf) to make predictions on the test data (X_test).\n",
    "# The 'predict' method of clf is used to predict the labels for each sample in X_test.\n",
    "\n",
    "# X_test contains the features of the unseen test data. \n",
    "# This data was set aside during the train-test split and was not used in training the model.\n",
    "# The model will use the patterns it learned during training to predict the labels for this new data.\n",
    "\n",
    "# The predictions made by the model are stored in the variable Y_preds.\n",
    "# Y_preds will be a numpy array containing the predicted labels for each sample in X_test.\n",
    "\n",
    "# Y_preds\n",
    "# This line when executed in a Jupyter Notebook will display the contents of Y_preds.\n",
    "# It shows the predictions made by the model for the test set.\n",
    "# These predicted labels can be compared with the actual labels (Y_test) to evaluate the model's performance.\n",
    "\n",
    "# By comparing Y_preds with Y_test, you can assess how well your model is performing.\n",
    "# Common ways to evaluate classification models include accuracy, precision, recall, and the confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f8dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate the Model\n",
    "clf.score(X_train, Y_train)\n",
    "\n",
    "# Evaluating the Model on the Training Set\n",
    "\n",
    "# clf.score(X_train, Y_train)\n",
    "# This line evaluates the performance of your RandomForestClassifier model on the training data.\n",
    "# The 'score' method returns the accuracy of the model, which is the proportion of correct predictions.\n",
    "\n",
    "# X_train and Y_train are the features and labels of the training set, respectively.\n",
    "# The model was trained on this data, so this score tells you how well the model fits the training data.\n",
    "\n",
    "# However, it's important to note that evaluating the model on the training data can be misleading.\n",
    "# A high score on the training data might simply mean that the model has memorized the \n",
    "# training data (overfitting),\n",
    "# rather than learning the underlying patterns in the data.\n",
    "\n",
    "# A more accurate assessment of the model's performance is obtained by evaluating it on the \n",
    "# test set (X_test and Y_test),\n",
    "# which consists of data that the model hasn't seen during training.\n",
    "# This helps in understanding how well the model generalizes to new, unseen data.\n",
    "\n",
    "# Generally, in machine learning, it's recommended to look at both the training score and the test score.\n",
    "# A good model will have high scores on both the training and testing datasets.\n",
    "# If the model performs well on the training data but poorly on the test data, \n",
    "# it's an indication that the model may be overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9198b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test,Y_test)\n",
    "\n",
    "# Evaluating the Model on the Test Set\n",
    "\n",
    "# clf.score(X_test, Y_test)\n",
    "# This line evaluates the performance of your RandomForestClassifier model on the test data.\n",
    "# The 'score' method, in this case, returns the accuracy of the model for the test set,\n",
    "# which is the proportion of correct predictions out of all predictions made on the test set.\n",
    "\n",
    "# X_test and Y_test are the features and labels of the test set, respectively.\n",
    "# The test set is crucial for evaluating the model because it consists of data that the model\n",
    "# has not seen during training.\n",
    "# This helps in understanding how well the model generalizes to new, unseen data.\n",
    "\n",
    "# A high score on the test set indicates that the model not only learned the patterns in the training data,\n",
    "# but it is also able to apply these patterns to make accurate predictions on new data.\n",
    "\n",
    "# It's important to compare the model's performance on the training set (evaluated previously) and the test set.\n",
    "# Ideally, the model should perform well on both sets. \n",
    "# A large discrepancy between training and test scores might indicate issues such as overfitting \n",
    "# (if the training score is much higher) \n",
    "# or underfitting (if the training score is too low compared to the test score).\n",
    "\n",
    "# The accuracy obtained here gives a quick overview of how well the model performs, \n",
    "# but it's also useful to look at other metrics like precision, recall, F1 score, and confusion matrices\n",
    "# for a more comprehensive evaluation, especially in cases where the dataset is imbalanced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More ways to evaluate the model.\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
    "\n",
    "# This below will return the compararison of the test label and the predication label.\n",
    "print(classification_report(Y_test, Y_preds))\n",
    "\n",
    "\n",
    "# Generating a Classification Report for Model Evaluation\n",
    "\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "# This line imports various evaluation metrics from scikit-learn's metrics module,\n",
    "# including classification_report, which we'll use to evaluate the model.\n",
    "\n",
    "# Using classification_report to Evaluate the Model\n",
    "# print(classification_report(Y_test, Y_preds))\n",
    "# This line generates and prints out the classification report for your model based on the test data.\n",
    "# The classification report provides detailed performance metrics for each class.\n",
    "\n",
    "# Y_test: These are the true labels from the test set.\n",
    "# Y_preds: These are the predicted labels by your model.\n",
    "\n",
    "# The classification report includes several important metrics:\n",
    "# - Precision: Measures the accuracy of positive predictions for each class.\n",
    "# - Recall: Measures the ability of the model to find all the positive samples for each class.\n",
    "# - F1-score: Provides a balance between precision and recall. It's a harmonic mean of the two.\n",
    "# - Support: Indicates the number of actual occurrences of each class in the specified dataset.\n",
    "\n",
    "# These metrics are provided for each class in your target variable and include averages,\n",
    "# giving you a detailed overview of how well the model is performing for each type of classification.\n",
    "# This detailed breakdown can help in identifying if the model is underperforming for any particular class \n",
    "# and assist in further refining the model or addressing any data imbalances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290302b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test,Y_preds)\n",
    "\n",
    "# Generating a Confusion Matrix for Model Evaluation\n",
    "\n",
    "# confusion_matrix(Y_test, Y_preds)\n",
    "# This line generates the confusion matrix for your model based on the test data.\n",
    "# A confusion matrix is a table often used to describe the performance of a classification model.\n",
    "\n",
    "# Y_test: These are the true labels from the test set.\n",
    "# Y_preds: These are the predicted labels by your model.\n",
    "\n",
    "# The confusion matrix compares the actual target values with those predicted by the model,\n",
    "# providing a detailed breakdown of:\n",
    "# - True Positives (TP): Correctly predicted positive observations\n",
    "# - True Negatives (TN): Correctly predicted negative observations\n",
    "# - False Positives (FP): Incorrectly predicted positive observations (Type I error)\n",
    "# - False Negatives (FN): Incorrectly predicted negative observations (Type II error)\n",
    "\n",
    "# Each row of the matrix represents the instances in an actual class, \n",
    "# while each column represents the instances in a predicted class, or vice versa. \n",
    "# This setup allows you to see the types of errors (if any) your model is making.\n",
    "\n",
    "# Interpreting the confusion matrix can provide insights into not only the overall performance of the model\n",
    "# but also into how it performs on each individual class. It can be particularly useful to identify \n",
    "# any biases the model may have towards certain classes and can inform how you might improve the model,\n",
    "# perhaps by providing more data for underrepresented classes or by tweaking the model itself.\n",
    "\n",
    "# Generally, a high number of True Positives and True Negatives and low numbers of False Positives \n",
    "# and False Negatives\n",
    "# are indicative of good model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f145b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y_test,Y_preds)\n",
    "\n",
    "# Calculating the Accuracy Score for Model Evaluation\n",
    "\n",
    "# accuracy_score(Y_test, Y_preds)\n",
    "# This line calculates the accuracy of the model based on the test data.\n",
    "# Accuracy is one of the most common metrics used to evaluate classification models.\n",
    "\n",
    "# Y_test: These are the true labels from the test set.\n",
    "# Y_preds: These are the predicted labels by your model.\n",
    "\n",
    "# The accuracy score is the ratio of correct predictions to total predictions made:\n",
    "# Accuracy = (True Positives + True Negatives) / Total Predictions\n",
    "\n",
    "# It represents how often the classifier is correct overall across all classes.\n",
    "# In simple terms, it answers the question, \"Out of all the classifications, how many did the model get right?\"\n",
    "\n",
    "# While accuracy is a useful metric, it should be considered alongside other metrics like precision, recall, \n",
    "# and the F1 score, especially in scenarios where the data is imbalanced or when different types of \n",
    "# errors have different costs.\n",
    "\n",
    "# A high accuracy score indicates that the model has a high rate of correctly predicting both\n",
    "# positive and negative classes.\n",
    "# However, don't rely solely on accuracy if the dataset is imbalanced (i.e., one class is much \n",
    "# more frequent than others).\n",
    "# In such cases, the model might just predict the most common class most of the time and still \n",
    "# achieve high accuracy.\n",
    "\n",
    "# Typically, after getting an overall sense of the model's performance through accuracy, \n",
    "# you would dive deeper into the performance details using the confusion matrix and classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533fd091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Improve the Model.\n",
    "# Try diferent amounts of the n_estimators\n",
    "np.random.seed(42)\n",
    "for i in range (10,100,10):\n",
    "    print(f\"Trying model with {i} estimators....\")\n",
    "    clf = RandomForestClassifier(n_estimators=i).fit(X_train,Y_train)\n",
    "    print(f\"Model accuracy on the test set: {clf.score(X_test,Y_test) * 100:.2f}%\")\n",
    "    print(\"\")      \n",
    "\n",
    "# Improving the Model by Tuning n_estimators in RandomForestClassifier\n",
    "\n",
    "# Trying different amounts of n_estimators\n",
    "# np.random.seed(42)\n",
    "# Setting a random seed ensures the results are reproducible.\n",
    "\n",
    "# for i in range(10, 100, 10):\n",
    "# This loop iterates through different values for 'n_estimators' starting from 10 up to 90, \n",
    "# increasing by 10 each time.\n",
    "# 'n_estimators' in a RandomForestClassifier refers to the number of trees in the forest.\n",
    "\n",
    "# print(f\"Trying model with {i} estimators....\")\n",
    "# This prints out the number of estimators the model is using in the current iteration.\n",
    "\n",
    "# clf = RandomForestClassifier(n_estimators=i).fit(X_train, Y_train)\n",
    "# Here, a new RandomForestClassifier is created with the current number of estimators (i) \n",
    "# and fitted to the training data.\n",
    "# This is done within the loop, so the classifier is retrained for each different value of 'n_estimators'.\n",
    "\n",
    "# print(f\"Model accuracy on the test set: {clf.score(X_test, Y_test) * 100:.2f}%\")\n",
    "# After fitting the model, this line prints out the accuracy of the classifier on the test data.\n",
    "# Multiplying by 100 converts it into a percentage, and :.2f formats the number to two decimal places.\n",
    "\n",
    "# print(\"\")  \n",
    "# This just prints a new line for better readability between each iteration's results.\n",
    "\n",
    "# By iterating through different values of 'n_estimators', you can observe how changing the number of trees\n",
    "# impacts the model's accuracy on the test set. \n",
    "# The goal is to identify the number of trees that provides the best trade-off between performance\n",
    "# and computational efficiency.\n",
    "\n",
    "# It's important to note that more trees in the forest doesn't always mean a better model. \n",
    "# After a certain point, increasing the number of trees may not significantly improve the \n",
    "# model's performance and can even \n",
    "# lead to longer training times. This experiment helps to find an optimal point or at least a\n",
    "# range of 'n_estimators' \n",
    "# that offers good model accuracy without unnecessary computational cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Save a model and load it\n",
    "import pickle\n",
    "\n",
    "pickle.dump(clf, open(\"random_forest_model_1.pk1\", \"wb\"))\n",
    "\n",
    "# Saving the Trained Model for Later Use\n",
    "\n",
    "# import pickle\n",
    "# 'pickle' is a Python module used to serialize and deserialize Python objects.\n",
    "# Serialization is the process of converting a Python object into a byte stream,\n",
    "# and deserialization is the reverse process. It's useful for saving models and other data structures.\n",
    "\n",
    "# pickle.dump(clf, open(\"random_forest_model_1.pk1\", \"wb\"))\n",
    "# This line saves the trained RandomForestClassifier (clf) to a file.\n",
    "\n",
    "# clf: This is the trained RandomForestClassifier model that you want to save.\n",
    "\n",
    "# open(\"random_forest_model_1.pk1\", \"wb\"): This opens a file named 'random_forest_model_1.pk1'\n",
    "#  in binary write mode ('wb').\n",
    "# If the file doesn't exist, it will be created. If it does exist, it will be overwritten.\n",
    "\n",
    "# The 'dump' function of pickle is used to serialize the 'clf' object and write it to the file.\n",
    "# This saved model can be loaded later to make predictions without needing to retrain the model.\n",
    "\n",
    "# Saving the model is particularly useful if the training process is time-consuming,\n",
    "# or if you want to deploy the model for use in applications, share it with others, or simply keep \n",
    "# a version of the model \n",
    "# that you can return to later.\n",
    "\n",
    "# Note: It's important to remember that the version of scikit-learn (or any other libraries used) \n",
    "# should be the same when you save the model and when you load it, as changes in the library versions \n",
    "# might not be compatible with the model file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92070459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets load the model. \"rb\" stands for read binaries\n",
    "loded_model = pickle.load(open(\"random_forest_model_1.pk1\", \"rb\"))\n",
    "\n",
    "# score the model .\n",
    "\n",
    "loded_model.score(X_test,Y_test)\n",
    "\n",
    "# Loading the Saved Model and Evaluating It\n",
    "\n",
    "# Loading the model\n",
    "# loded_model = pickle.load(open(\"random_forest_model_1.pk1\", \"rb\"))\n",
    "# This line loads the previously saved model into the variable 'loaded_model'.\n",
    "# 'pickle.load()' is used to deserialize the object, converting the byte stream back into a Python object.\n",
    "\n",
    "# open(\"random_forest_model_1.pk1\", \"rb\"): This opens the file named 'random_forest_model_1.pk1' \n",
    "# in binary read mode ('rb').\n",
    "# The file must exist in the directory you're working in, or you should provide the correct path to the file.\n",
    "\n",
    "# \"rb\" stands for \"read binary\", which is necessary because the model was saved in a binary format.\n",
    "\n",
    "# Once loaded, 'loaded_model' is essentially a clone of the 'clf' object that was saved earlier.\n",
    "# It retains all the properties, parameters, and learned patterns of the original trained model.\n",
    "\n",
    "# Scoring the loaded model\n",
    "# loaded_model.score(X_test, Y_test)\n",
    "# After loading the model, you might want to confirm that it's still performing as expected.\n",
    "# This line uses the 'score' method to evaluate the loaded model's accuracy, just like you did with \n",
    "# the original 'clf' model.\n",
    "\n",
    "# X_test and Y_test are the same test set features and labels used to evaluate the original model.\n",
    "# This line effectively gives you the accuracy of the model on the test set, \n",
    "# allowing you to verify that the model has been loaded correctly and is functioning as expected.\n",
    "\n",
    "# Using 'loaded_model.score()' is a quick way to ensure that the deserialization process \n",
    "# has worked correctly and that the model is ready to be used for predictions or further evaluation.\n",
    "\n",
    "# This process is particularly useful in operational settings where you might train a model in one\n",
    "# script or notebook,\n",
    "# save it, and then load it in a different script, application, or system to make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb192e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "What_we_are_going_to_cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c3201c",
   "metadata": {},
   "source": [
    "## 1. getting our data ready to be used with machine learning\n",
    "\n",
    "Three main things we have to do.\n",
    "\n",
    "1.Split the data into features and labels (usually 'x' and 'y')\n",
    "\n",
    "2.Filling (also called inputing) or disregarding missing values\n",
    "\n",
    "3.Converting non-numerical values to numerical values (also called feature encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c86f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc66cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets drop target or remove target from the table\n",
    "\n",
    "x=heart_disease.drop(\"target\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db542d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the y axis will be target \n",
    "y=heart_disease[\"target\"]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88212ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c9713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 242 came from spliting\n",
    "# 13 came from the number of coulmns\n",
    "x.shape[0] * 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707c558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(heart_disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d02595b",
   "metadata": {},
   "source": [
    "### 1.1  Make sure its all numerical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc1f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales =pd.read_csv(\"car-sales-extended.csv\")\n",
    "car_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a24db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(car_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96857b79-963c-434b-81c3-3cdcf01529cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7269f3b-6eeb-450d-8d97-4f2724357113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X/Y\n",
    "\n",
    "X = car_sales.drop(\"Price\", axis=1)\n",
    "Y = car_sales[\"Price\"]\n",
    "\n",
    "# Split into training and test\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7ad12-db4b-47f5-9475-c1c961d1443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build machine learning model\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, Y_train)\n",
    "model.score(X_test,Y_test)\n",
    "\n",
    "# will get Value error if you do not convert  strings into int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce914b8c-223b-4e40-b641-e0d4da422405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we convert our data in to numbers\n",
    "# Turn the catagories into numbers\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "catagorical_features = [\"Make\", \"Colour\",\"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\",one_hot,catagorical_features)],remainder=\"passthrough\")\n",
    "\n",
    "transformed_X = transformer.fit_transform(X)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84de715-4f84-474f-99e2-cc97aea5b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it in a data frame\n",
    "pd.DataFrame(transformed_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6554a7-e503-4b4e-9d22-e03218ba89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A second way to change to number\n",
    "\n",
    "dummies = pd.get_dummies(car_sales[[\"Make\", \"Colour\",\"Doors\"]])\n",
    "dummies \n",
    "\n",
    "# A second way to change to number\n",
    "\n",
    "# dummies = pd.get_dummies(car_sales[[\"Make\", \"Colour\",\"Doors\"]])\n",
    "# This line creates dummy variables for categorical data in the car_sales DataFrame.\n",
    "# 'pd.get_dummies()' is a function from the pandas library that converts categorical variable(s)\n",
    "# into dummy/indicator variables.\n",
    "\n",
    "# What are dummy variables?\n",
    "# - Dummy variables are binary (0 or 1) columns created to represent each unique value in a \n",
    "# categorical variable.\n",
    "# - For each unique category in the variable, a new column is created where 1 indicates the\n",
    "# presence of the category and 0 indicates the absence.\n",
    "\n",
    "# car_sales[[\"Make\", \"Colour\", \"Doors\"]]\n",
    "# - This part is selecting the 'Make', 'Colour', and 'Doors' columns from the car_sales DataFrame.\n",
    "# - These are typically categorical columns where 'Make' might be car brands, 'Colour' could be the \n",
    "# color of the car, \n",
    "#   and 'Doors' the number of doors (which might be treated as categorical, e.g., '2-door', '4-door', etc.).\n",
    "\n",
    "# dummies = ...\n",
    "# - The resulting dummy variables are stored in a new DataFrame called 'dummies'.\n",
    "# - This DataFrame now contains several columns, each representing a unique value from the original\n",
    "# columns as binary indicators.\n",
    "\n",
    "# dummies\n",
    "# - This line, when executed, displays the DataFrame 'dummies'.\n",
    "# - It's useful to visualize the new dummy variables to understand how the original categorical \n",
    "# data has been transformed.\n",
    "\n",
    "# Why use dummy variables?\n",
    "# - Many machine learning models require numerical input and do not handle categorical data natively.\n",
    "# - Dummy variables allow for the representation of categorical data in a binary, numerical format\n",
    "# that models can understand.\n",
    "# - It avoids the issue of assigning ordinal numbers to categories which might lead the model to assume \n",
    "# a natural order among categories.\n",
    "\n",
    "# Note: After creating dummy variables, it's important to consider multicollinearity, \n",
    "# which can occur due to the dummy variable trap (one dummy variable can be predicted from others).\n",
    "# A common practice is to drop one dummy column for each original categorical variable to avoid this issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97199cf-ad0a-407b-a4cb-81fc61364fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's refit the model\n",
    "\n",
    "np.random.seed(42)\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(transformed_X,Y, test_size=0.2)\n",
    "\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a45a92-ec49-4ebc-9112-be061bc10c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b792fda-ca90-4e24-8777-0ae74c506062",
   "metadata": {},
   "source": [
    "## 1.2  Dealing with missing data\n",
    "\n",
    "1. Fill them with some value (also known as imputaion)\n",
    "2.  Remove the sample with missing data altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "539e6991-a244-4812-a264-a5d9595b9b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Make</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Odometer (KM)</th>\n",
       "      <th>Doors</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>35431.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>Blue</td>\n",
       "      <td>192714.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19943.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>84714.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>White</td>\n",
       "      <td>154365.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13434.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>Blue</td>\n",
       "      <td>181577.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14043.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Make Colour  Odometer (KM)  Doors    Price\n",
       "0   Honda  White        35431.0    4.0  15323.0\n",
       "1     BMW   Blue       192714.0    5.0  19943.0\n",
       "2   Honda  White        84714.0    4.0  28343.0\n",
       "3  Toyota  White       154365.0    4.0  13434.0\n",
       "4  Nissan   Blue       181577.0    3.0  14043.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import car sales missing data\n",
    "\n",
    "car_sales_missing = pd.read_csv(\"car-sales-extended-missing-data.csv\")\n",
    "car_sales_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "663719fe-0112-4393-a2bd-d25dbe7c884f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             49\n",
       "Colour           50\n",
       "Odometer (KM)    50\n",
       "Doors            50\n",
       "Price            50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this code is intended to give you a summary count of all missing (NaN) \n",
    "# values in each column of the car_sales_missing DataFrame.\n",
    "\n",
    "car_sales_missing.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a85a92bc-248b-424a-9e8b-7f6662ce91f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X & Y\n",
    "\n",
    "x = car_sales_missing.drop(\"Price\", axis=1)\n",
    "y = car_sales_missing[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "218c5d55-e560-4625-a8a1-50b4116dcbf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A given column is not a column of the dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Make'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\utils\\__init__.py:447\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[1;32m--> 447\u001b[0m     col_idx \u001b[38;5;241m=\u001b[39m \u001b[43mall_columns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_idx, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Make'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m one_hot \u001b[38;5;241m=\u001b[39m OneHotEncoder()\n\u001b[0;32m      8\u001b[0m transformer \u001b[38;5;241m=\u001b[39m ColumnTransformer([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone_hot\u001b[39m\u001b[38;5;124m\"\u001b[39m,one_hot,catagorical_features)],remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m transformed_X \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m transformed_X\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:740\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[1;32m--> 740\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[0;32m    743\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(X, y, _fit_transform_one)\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:448\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    446\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[0;32m    447\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[1;32m--> 448\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\utils\\__init__.py:455\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    452\u001b[0m             column_indices\u001b[38;5;241m.\u001b[39mappend(col_idx)\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 455\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA given column is not a column of the dataframe\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column_indices\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: A given column is not a column of the dataframe"
     ]
    }
   ],
   "source": [
    "#  Let's try and conver them into numbers\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "catagorical_features = [\"Make\", \"Colour\",\"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\",one_hot,catagorical_features)],remainder=\"passthrough\")\n",
    "\n",
    "transformed_X = transformer.fit_transform(X)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d90eb2-508e-417b-ae05-cf930c0a1910",
   "metadata": {},
   "source": [
    "### 1: Fill Missing Data with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "de39afb2-41f2-4b5b-85c7-02de3be86d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code is using the fillna() method from pandas to handle missing values in a DataFrame called car_sales_missing. Here's what each line is doing:\n",
    "\n",
    "# Filling \"Make\" Column: It replaces all missing (NaN) values in the \"Make\" column with the string \"missing\".\n",
    "\n",
    "# Filling \"Colour\" Column: It replaces all missing (NaN) values in the \"Colour\" column with the string \"missing\".\n",
    "\n",
    "# Filling \"Odometer (KM)\" Column: It replaces all missing (NaN) values in the \"Odometer (KM)\" column with \n",
    "# the mean (average) of the existing values in that column.\n",
    "\n",
    "# Filling \"Doors\" Column: It replaces all missing (NaN) values in the \"Doors\" column with the number 4.\n",
    "\n",
    "# In all cases, the inplace=True parameter updates the DataFrame directly, meaning it changes the original\n",
    "# car_sales_missing DataFrame rather than creating a new one. This is a common way to handle missing\n",
    "# data by assigning default or calculated values to ensure the dataset is complete for analysis \n",
    "# or further processing.\n",
    "\n",
    "\n",
    "# Fill the \"make \" column\n",
    "\n",
    "car_sales_missing[\"Make\"].fillna(\"missing\", inplace=True)\n",
    "\n",
    "# Fill the \"Colour\" column\n",
    "\n",
    "car_sales_missing[\"Colour\"].fillna(\"missing\", inplace=True)\n",
    "\n",
    "# Fill the \"Odometer (KM)\" column\n",
    "\n",
    "car_sales_missing[\"Odometer (KM)\"].fillna(car_sales_missing[\"Odometer (KM)\"].mean(), inplace=True)\n",
    "\n",
    "# Fill the \"Doors\" column\n",
    "\n",
    "car_sales_missing[\"Doors\"].fillna(4, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f653502c-4692-4942-b62f-137407c5e89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             0\n",
       "Colour           0\n",
       "Odometer (KM)    0\n",
       "Doors            0\n",
       "Price            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check our data frame again\n",
    "\n",
    "# Price column is left out in purpose as its the one we are trying to predict.\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "58a1bb62-c176-4e2a-af90-4e42c3b66367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with missing price value\n",
    "car_sales_missing.dropna(inplace=True)\n",
    "#  The code car_sales_missing.dropna(inplace=True) removes all rows in the car_sales_missing \n",
    "# DataFrame that contain any missing values (NaN) and updates the DataFrame in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cfe6c89e-7a55-4992-ada7-840b7e9e4c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             0\n",
       "Colour           0\n",
       "Odometer (KM)    0\n",
       "Doors            0\n",
       "Price            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use .isna() and . sum() to get the sum of all missing values\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b700af39-028b-437e-85eb-77846893656e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "950"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(car_sales_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "282f223b-51da-4eb3-b4c1-b3f6f60c2578",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = car_sales_missing.drop(\"Price\", axis=1)\n",
    "y = car_sales_missing[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "23964a10-0424-4467-ba17-2ff443131818",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A given column is not a column of the dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Make'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\utils\\__init__.py:447\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[1;32m--> 447\u001b[0m     col_idx \u001b[38;5;241m=\u001b[39m \u001b[43mall_columns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_idx, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Make'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m one_hot \u001b[38;5;241m=\u001b[39m OneHotEncoder()\n\u001b[0;32m      6\u001b[0m transformer \u001b[38;5;241m=\u001b[39m ColumnTransformer([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone_hot\u001b[39m\u001b[38;5;124m\"\u001b[39m,one_hot,catagorical_features)],remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m transformed_X \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m transformed_X\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:740\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[1;32m--> 740\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[0;32m    743\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(X, y, _fit_transform_one)\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:448\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    446\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[0;32m    447\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[1;32m--> 448\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\utils\\__init__.py:455\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    452\u001b[0m             column_indices\u001b[38;5;241m.\u001b[39mappend(col_idx)\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 455\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA given column is not a column of the dataframe\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column_indices\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: A given column is not a column of the dataframe"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "catagorical_features = [\"Make\", \"Colour\",\"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\",one_hot,catagorical_features)],remainder=\"passthrough\")\n",
    "\n",
    "transformed_X = transformer.fit_transform(X)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a8a1d-86e9-4f3e-872d-589b0843f64e",
   "metadata": {},
   "source": [
    "### Option 2 : Fill missing values with Sickit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4c3a33bc-3374-4138-a1c0-00329510a022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Make</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Odometer (KM)</th>\n",
       "      <th>Doors</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>35431.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>Blue</td>\n",
       "      <td>192714.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19943.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>84714.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>White</td>\n",
       "      <td>154365.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13434.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>Blue</td>\n",
       "      <td>181577.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14043.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Make Colour  Odometer (KM)  Doors    Price\n",
       "0   Honda  White        35431.0    4.0  15323.0\n",
       "1     BMW   Blue       192714.0    5.0  19943.0\n",
       "2   Honda  White        84714.0    4.0  28343.0\n",
       "3  Toyota  White       154365.0    4.0  13434.0\n",
       "4  Nissan   Blue       181577.0    3.0  14043.0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataframe which has missing vales and inspect top five values.\n",
    "car_sales_missing = pd.read_csv(\"car-sales-extended-missing-data.csv\")\n",
    "car_sales_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fb23a7f2-680c-4d80-a35e-647527dac156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             49\n",
       "Colour           50\n",
       "Odometer (KM)    50\n",
       "Doors            50\n",
       "Price            50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use .isna() and . sum() to get the sum of all missing values\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "15e3e95f-5da1-46d5-84cd-46e2f86d512f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             47\n",
       "Colour           46\n",
       "Odometer (KM)    48\n",
       "Doors            47\n",
       "Price             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we are dropping na values in the Price column \n",
    "# Then recaluculate how much are missing.\n",
    "car_sales_missing.dropna(subset=[\"Price\"], inplace=True)\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0768acec-e116-4f6e-a72e-346c2212b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into X & Y \n",
    "x = car_sales_missing.drop(\"Price\", axis=1)\n",
    "y = car_sales_missing[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "977672e5-f6be-4efd-a0e5-cbf90cd933a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A given column is not a column of the dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Make'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\utils\\__init__.py:447\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[1;32m--> 447\u001b[0m     col_idx \u001b[38;5;241m=\u001b[39m \u001b[43mall_columns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_idx, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Make'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 27\u001b[0m\n\u001b[0;32m     19\u001b[0m imputer \u001b[38;5;241m=\u001b[39m ColumnTransformer([\n\u001b[0;32m     20\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat_imputer\u001b[39m\u001b[38;5;124m\"\u001b[39m, cat_imputer, cat_features),  \u001b[38;5;66;03m# Apply categorical imputer to cat_features\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoor_imputer\u001b[39m\u001b[38;5;124m\"\u001b[39m, door_imputer, door_feature),  \u001b[38;5;66;03m# Apply door imputer to door_feature\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_imputer\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_imputer, num_features)  \u001b[38;5;66;03m# Apply numerical imputer to num_features\u001b[39;00m\n\u001b[0;32m     23\u001b[0m ])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Assume X is your dataframe\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Apply the ColumnTransformer to the data\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m filled_X \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Imputes missing values in X\u001b[39;00m\n\u001b[0;32m     28\u001b[0m filled_X\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:740\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[1;32m--> 740\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[0;32m    743\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(X, y, _fit_transform_one)\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:448\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    446\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[0;32m    447\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[1;32m--> 448\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\utils\\__init__.py:455\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    452\u001b[0m             column_indices\u001b[38;5;241m.\u001b[39mappend(col_idx)\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 455\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA given column is not a column of the dataframe\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column_indices\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: A given column is not a column of the dataframe"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Imputer are used here to fill missing data\n",
    "# Setup imputers: one for categorical, one for door number, and one for numerical features\n",
    "cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"missing\")  # Fills missing categorical values with 'missing'\n",
    "door_imputer = SimpleImputer(strategy=\"constant\", fill_value=4)  # Fills missing door values with '4'\n",
    "num_imputer = SimpleImputer(strategy=\"mean\")  # Fills missing numerical values with mean of the column\n",
    "\n",
    "# Define feature lists for different data types\n",
    "cat_features = [\"Make\", \"Colour\"]  # Categorical feature names\n",
    "door_feature = [\"Doors\"]  # Door feature name\n",
    "num_features = [\"Odometer (KM)\"]  # Numerical feature names\n",
    "\n",
    "# Create a ColumnTransformer to apply each imputer to the correct features\n",
    "# ColumnTransformer takes a list of multiple transformers \n",
    "# Each element of the list is a tuple, specifying the transformer to apply to a specific subset of columns. \n",
    "# This is powerful because it allows different treatments for different types of data\n",
    "imputer = ColumnTransformer([\n",
    "    (\"cat_imputer\", cat_imputer, cat_features),  # Apply categorical imputer to cat_features\n",
    "    (\"door_imputer\", door_imputer, door_feature),  # Apply door imputer to door_feature\n",
    "    (\"num_imputer\", num_imputer, num_features)  # Apply numerical imputer to num_features\n",
    "])\n",
    "\n",
    "# Assume X is your dataframe\n",
    "# Apply the ColumnTransformer to the data\n",
    "filled_X = imputer.fit_transform(X)  # Imputes missing values in X\n",
    "filled_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e4432e86-a7c8-4204-a93c-9c7c6fcf3c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Make</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Doors</th>\n",
       "      <th>Odometer (KM)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>4</td>\n",
       "      <td>35431.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>Blue</td>\n",
       "      <td>5</td>\n",
       "      <td>192714.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>4</td>\n",
       "      <td>84714.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>White</td>\n",
       "      <td>4</td>\n",
       "      <td>154365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>Blue</td>\n",
       "      <td>3</td>\n",
       "      <td>181577.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Make Colour Doors Odometer (KM)\n",
       "0   Honda  White     4       35431.0\n",
       "1     BMW   Blue     5      192714.0\n",
       "2   Honda  White     4       84714.0\n",
       "3  Toyota  White     4      154365.0\n",
       "4  Nissan   Blue     3      181577.0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check the filled column \n",
    "car_sales_filled = pd.DataFrame(filled_X,\n",
    "                              columns=[\"Make\",\"Colour\",\"Doors\", \"Odometer (KM)\"])\n",
    "\n",
    "car_sales_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9def9b05-2d60-4bf8-8530-4b3a3ce664ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             0\n",
       "Colour           0\n",
       "Doors            0\n",
       "Odometer (KM)    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see here that we have no missing values\n",
    "car_sales_filled.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "130450b5-f0c1-406d-9265-52fbea2126b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 0.0, ..., 1.0, 0.0, 35431.0],\n",
       "       [1.0, 0.0, 0.0, ..., 0.0, 1.0, 192714.0],\n",
       "       [0.0, 1.0, 0.0, ..., 1.0, 0.0, 84714.0],\n",
       "       ...,\n",
       "       [0.0, 0.0, 1.0, ..., 1.0, 0.0, 66604.0],\n",
       "       [0.0, 1.0, 0.0, ..., 1.0, 0.0, 215883.0],\n",
       "       [0.0, 0.0, 0.0, ..., 1.0, 0.0, 248360.0]], dtype=object)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we convert our data in to numbers\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "catagorical_features = [\"Make\", \"Colour\",\"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\",one_hot,catagorical_features)],remainder=\"passthrough\")\n",
    "\n",
    "transformed_X = transformer.fit_transform(car_sales_filled)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2817cb9b-0040-4097-a47c-41fe3d3cdf2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1000, 303]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m---> 11\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m=\u001b[39m RandomForestRegressor()\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train,Y_train)\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2614\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2612\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2614\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2616\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2617\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2619\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\utils\\validation.py:455\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    454\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 455\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\utils\\validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    407\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    412\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1000, 303]"
     ]
    }
   ],
   "source": [
    "# Now that we have filled missing data and convertedd to numbers \n",
    "# lets fit a model.\n",
    "\n",
    "np.random.seed(42)\n",
    "#This line sets the seed for NumPy's random number generator to '42', \n",
    "# ensuring that the results are reproducible.\n",
    "#(and the train-test split) will produce the same results each run.\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(transformed_X,\n",
    "                                                   Y,\n",
    "                                                   test_size=0.2)\n",
    "\n",
    "model= RandomForestRegressor()\n",
    "model.fit(X_train,Y_train)\n",
    "model.score(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "38ec1a3f-9b23-4e45-be2f-84e27f3174a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(car_sales_filled), len(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b5a0b-dcbe-4fa4-8d93-50988fb5f1ce",
   "metadata": {},
   "source": [
    "## Choosing the right model for your problem.\n",
    "\n",
    "Somethings to note:\n",
    "\n",
    "* SKlearn refers to machine learning models, algorithms as estimators.\n",
    "* Classification problem - predicting a catagory (heart disease or not)\n",
    "* sometimes you will see clf(short for classifier) used as a classification estimator\n",
    "* Regrassion problem - predicting a number(selling price of a car)\n",
    "* Clustering: Groups similar items into clusters to uncover inherent structures; for example, customer  segmentation in marketing to target customers with similar buying habits.\n",
    "* Dimensionality Reduction: Reduces the number of variables in high-dimensional data, improving visualization and efficiency; for example, reducing features in a dataset for facial recognition to improve model performance and reduce complexity.\n",
    "* Sklearn machine learning map:  https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8463067d-548a-49a7-bc00-36778c4158d4",
   "metadata": {},
   "source": [
    "### Picking a machine learning\n",
    "Let\"s use the California Housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6197b23c-1e35-4911-a25c-378bd8870cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
       "           37.88      , -122.23      ],\n",
       "        [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
       "           37.86      , -122.22      ],\n",
       "        [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
       "           37.85      , -122.24      ],\n",
       "        ...,\n",
       "        [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
       "           39.43      , -121.22      ],\n",
       "        [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
       "           39.43      , -121.32      ],\n",
       "        [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
       "           39.37      , -121.24      ]]),\n",
       " 'target': array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894]),\n",
       " 'frame': None,\n",
       " 'target_names': ['MedHouseVal'],\n",
       " 'feature_names': ['MedInc',\n",
       "  'HouseAge',\n",
       "  'AveRooms',\n",
       "  'AveBedrms',\n",
       "  'Population',\n",
       "  'AveOccup',\n",
       "  'Latitude',\n",
       "  'Longitude'],\n",
       " 'DESCR': '.. _california_housing_dataset:\\n\\nCalifornia Housing dataset\\n--------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 20640\\n\\n    :Number of Attributes: 8 numeric, predictive attributes and the target\\n\\n    :Attribute Information:\\n        - MedInc        median income in block group\\n        - HouseAge      median house age in block group\\n        - AveRooms      average number of rooms per household\\n        - AveBedrms     average number of bedrooms per household\\n        - Population    block group population\\n        - AveOccup      average number of household members\\n        - Latitude      block group latitude\\n        - Longitude     block group longitude\\n\\n    :Missing Attribute Values: None\\n\\nThis dataset was obtained from the StatLib repository.\\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\\n\\nThe target variable is the median house value for California districts,\\nexpressed in hundreds of thousands of dollars ($100,000).\\n\\nThis dataset was derived from the 1990 U.S. census, using one row per census\\nblock group. A block group is the smallest geographical unit for which the U.S.\\nCensus Bureau publishes sample data (a block group typically has a population\\nof 600 to 3,000 people).\\n\\nA household is a group of people residing within a home. Since the average\\nnumber of rooms and bedrooms in this dataset are provided per household, these\\ncolumns may take surprisingly large values for block groups with few households\\nand many empty houses, such as vacation resorts.\\n\\nIt can be downloaded/loaded using the\\n:func:`sklearn.datasets.fetch_california_housing` function.\\n\\n.. topic:: References\\n\\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\n      Statistics and Probability Letters, 33 (1997) 291-297\\n'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get California Housing dataset\n",
    "# The code fetches the California Housing dataset from scikit-learn and stores it in the variable housing,\n",
    "# providing data for machine learning tasks related to housing prices.\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "00a3afdb-fb0e-4a33-a792-1237257ce0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>1.5603</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.045455</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>845.0</td>\n",
       "      <td>2.560606</td>\n",
       "      <td>39.48</td>\n",
       "      <td>-121.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>2.5568</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.114035</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>356.0</td>\n",
       "      <td>3.122807</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-121.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>1.7000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.205543</td>\n",
       "      <td>1.120092</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>2.325635</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>1.8672</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.329513</td>\n",
       "      <td>1.171920</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2.123209</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>2.3886</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.254717</td>\n",
       "      <td>1.162264</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>2.616981</td>\n",
       "      <td>39.37</td>\n",
       "      <td>-121.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
       "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
       "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
       "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
       "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
       "\n",
       "       Longitude  \n",
       "0        -122.23  \n",
       "1        -122.22  \n",
       "2        -122.24  \n",
       "3        -122.25  \n",
       "4        -122.25  \n",
       "...          ...  \n",
       "20635    -121.09  \n",
       "20636    -121.21  \n",
       "20637    -121.22  \n",
       "20638    -121.32  \n",
       "20639    -121.24  \n",
       "\n",
       "[20640 rows x 8 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets change it into a dataframe\n",
    "housing_df = pd.DataFrame(housing[\"data\"], columns=housing[\"feature_names\"])\n",
    "housing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2d45ea99-21e4-4bb8-a448-6f33fbcc6e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  MedHouseVal  \n",
       "0    -122.23        4.526  \n",
       "1    -122.22        3.585  \n",
       "2    -122.24        3.521  \n",
       "3    -122.25        3.413  \n",
       "4    -122.25        3.422  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets add a target column \n",
    "housing_df[\"MedHouseVal\"] = housing[\"target\"]\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bb3e0f44-1656-45b1-a8a5-27476de182c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5758549611440126"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import algorithm using map\n",
    "# we will need to experiment to find the best algorithm\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create the data\n",
    "X = housing_df.drop(\"MedHouseVal\", axis= 1)\n",
    "Y = housing_df[\"MedHouseVal\"]  # median house price in 100,000s\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, Y_train,Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "\n",
    "# Instantiate and fit the model(on the training set)\n",
    "model = Ridge()\n",
    "model.fit(X_train,Y_train)\n",
    "\n",
    "# Check the score of the model(on teh test set)\n",
    "model.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d8a74e9b-3836-4f76-a6c2-4dee3a2da749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2841671821008396"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import algorithm using map\n",
    "# we will need to experiment to find the best algorithm\n",
    "# lets try another algoritm Lasso\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create the data\n",
    "X = housing_df.drop(\"MedHouseVal\", axis= 1)\n",
    "Y = housing_df[\"MedHouseVal\"]  # median house price in 100,000s\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, Y_train,Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "\n",
    "# Instantiate and fit the model(on the training set)\n",
    "model = Lasso()\n",
    "model.fit(X_train,Y_train)\n",
    "\n",
    "# Check the score of the model(on teh test set)\n",
    "model.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c4efdf-b86a-4ddb-8ae0-97c3666ac00a",
   "metadata": {},
   "source": [
    "When the algorithem we selected does not work or the score is not what we are looking for.\n",
    "We can always try a different model\n",
    "\n",
    "We can try ensembel model \n",
    "\n",
    "In scikit-learn, an ensemble model combines multiple machine learning models, like decision trees or classifiers, to improve prediction accuracy, stability, and robustness compared to using a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5be4cc0a-3144-41ad-858a-ea23a401d71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8065734772187598"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Import the  RandRandomForestRegressor model from the ensemble module\n",
    "from sklearn.ensemble import  RandomForestRegressor\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create the data \n",
    "X = housing_df.drop(\"MedHouseVal\", axis=1)\n",
    "Y = housing_df[\"MedHouseVal\"]\n",
    "\n",
    "# Split into train and test\n",
    "\n",
    "X_train, X_test , Y_train, Y_test = train_test_split(X,Y, test_size =0.2)\n",
    "\n",
    "# Create random forest model.\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# check the score of the model (on the test set)\n",
    "model.score(X_test, Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa3fc88-6ae6-4a71-ae49-fbb71b010f0b",
   "metadata": {},
   "source": [
    "## Picking a machine learnnig model for a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "df07a1a4-e741-439a-8047-9810490488bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_disease = pd.read_csv(\"heart-disease.csv\")\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6e050119-2db6-4489-a475-4e44d70f8a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(heart_disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316191e3-f762-464c-88ae-1509ecfe11ca",
   "metadata": {},
   "source": [
    "Going through the map we find that we can use LinearSVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "326c093d-c297-4a46-a375-1203f95e07d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8524590163934426"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the LinearSVC estimator class\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# make the data\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "Y = heart_disease[\"target\"]\n",
    "\n",
    "# split the data\n",
    "\n",
    "X_train, X_test,Y_train, Y_test  = train_test_split(X,Y, test_size = 0.2)\n",
    "\n",
    "# Instantiate LinearSVC \n",
    "\n",
    "Clf = LinearSVC(max_iter=100000)\n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "# evaluate the LinearSVC\n",
    "\n",
    "clf.score(X_test,Y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "75c6be1e-5c9e-47d9-95ba-c303b825a2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8524590163934426"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets compare it with another classifier\n",
    "# Import the RandomForestClassifier estimator class\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make the data\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "Y = heart_disease[\"target\"]\n",
    "\n",
    "# split the data\n",
    "\n",
    "X_train, X_test,Y_train, Y_test  = train_test_split(X,Y, test_size = 0.2)\n",
    "\n",
    "# Instantiate Random Forest Classifier\n",
    "\n",
    "Clf = RandomForestClassifier()\n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "# evaluate the RandomForestClassifier\n",
    "clf.score(X_test,Y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b93475-26d1-4e91-8c1f-91d647d6d457",
   "metadata": {},
   "source": [
    "Note:\n",
    "1. If you have structured data, use Ensemble methods.\n",
    "2.  If you have unstructured data, use deep learning or transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0785c41-ecc1-46bb-aae1-907852c226ff",
   "metadata": {},
   "source": [
    "##  Fit the model on our data and use it to make pridictions\n",
    "\n",
    "Note Diferent names for X and Y:\n",
    "* 'X' = features, feature variables data\n",
    "* 'Y'= labels, targets, target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6ac89dd9-52c3-4f35-9a47-aeb6ca4aca45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8524590163934426"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the RandomForestClassifer estimator class\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make the data\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "Y = heart_disease[\"target\"]\n",
    "\n",
    "# split the data\n",
    "\n",
    "X_train, X_test,Y_train, Y_test  = train_test_split(X,Y, test_size = 0.2)\n",
    "\n",
    "# Fit the model to the data\n",
    "\n",
    "Clf = RandomForestClassifier()\n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "# evaluate the RandomForestClassifier\n",
    "\n",
    "clf.score(X_test,Y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a612e3db-f8c6-464a-a090-b6dc7b4ceab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  \n",
       "0   0     1  \n",
       "1   0     2  \n",
       "2   0     2  \n",
       "3   0     2  \n",
       "4   0     2  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cbed310d-ea89-4f0f-b00c-b8aec0d4d38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0383e13-329e-4b0c-93d3-d4d0c7e06810",
   "metadata": {},
   "source": [
    "###  Make predictions using a machine learning model.\n",
    "\n",
    "2 ways to make predictions:\n",
    "\n",
    "1.predict()\n",
    "2.predict_proba()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "71236c87-793f-423f-9c1d-eae2dd4fdd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1. 3. 4. 5. 6.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use a trained model to make predictions\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:823\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    803\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 823\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    826\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:865\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    863\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    868\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:599\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    598\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 599\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\utils\\validation.py:940\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 940\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    941\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    942\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    943\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    944\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    945\u001b[0m         )\n\u001b[0;32m    947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    949\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    950\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    951\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1. 3. 4. 5. 6.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Use a trained model to make predictions\n",
    "clf.predict(np.array([1,3,4,5,6]))  # this does not work\n",
    "\n",
    "# \"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\n",
    "#    \"Reshape your data either using array.reshape(-1, 1) if \"\n",
    "#          \"your data has a single feature or array.reshape(1, -1) \"\n",
    "#           \"if it contains a single sample.\".format(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8e721ffb-5746-43c1-9c9a-9823fb5df7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have to use the same format , it has to be a 2d array.\n",
    "# we can use the X_test as our mode has not seen this data and is a same format.\n",
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c3205b28-0d23-45cf-aa61-cec5ae870a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is the reults from our data we had split earlier\n",
    "\n",
    "np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b8e385c2-2cbf-4bd6-abfc-28e28185789b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8524590163934426"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare predictions to the truth labels to evaluate the model.\n",
    "Y_preds = clf.predict(X_test)\n",
    "np.mean(Y_preds == Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "dd400ca5-b7ca-4048-8e38-1bb75c0bfc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8524590163934426"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the above result is thte same as the score we got earlier\n",
    "clf.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c0b746a1-1434-4441-82d6-1e4ae288e626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8524590163934426"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another way to score the model.\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test,Y_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265e9859-7b5f-4a69-b3e6-6a39e639714a",
   "metadata": {},
   "source": [
    "### Make prediction with predict_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "16fc2af9-afb9-4e10-adc6-aad64288375c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89, 0.11],\n",
       "       [0.49, 0.51],\n",
       "       [0.43, 0.57],\n",
       "       [0.84, 0.16],\n",
       "       [0.18, 0.82]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the classifier (clf) to predict probabilities of classification labels for the first 5 samples in X_test\n",
    "# The clf.predict_proba() function returns the predicted probabilities for each class\n",
    "predicted_probabilities = clf.predict_proba(X_test[:5])\n",
    "\n",
    "# Display the predicted probabilities\n",
    "predicted_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e756306d-1e11-4624-8f89-1887cfa4eacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets predict it the other way\n",
    "clf.predict(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8a746492-37e0-4acf-b859-b3e2cb3065a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>276</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>170</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>134</td>\n",
       "      <td>409</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "      <td>265</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "179   57    1   0       150   276    0        0      112      1      0.6   \n",
       "228   59    1   3       170   288    0        0      159      0      0.2   \n",
       "111   57    1   2       150   126    1        1      173      0      0.2   \n",
       "246   56    0   0       134   409    0        0      150      1      1.9   \n",
       "60    71    0   2       110   265    1        0      130      0      0.0   \n",
       "\n",
       "     slope  ca  thal  \n",
       "179      1   1     1  \n",
       "228      1   0     3  \n",
       "111      2   1     3  \n",
       "246      1   2     3  \n",
       "60       2   1     2  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "83fc4d90-af6e-4d31-a69d-65a90a4d8056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1    165\n",
       "0    138\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_disease[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef299c0-9496-42d5-a514-1b70e6fcaec6",
   "metadata": {},
   "source": [
    "### predict() can also be used for regression models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6e97a665-2138-479d-8365-5746151fa56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  MedHouseVal  \n",
       "0    -122.23        4.526  \n",
       "1    -122.22        3.585  \n",
       "2    -122.24        3.521  \n",
       "3    -122.25        3.413  \n",
       "4    -122.25        3.422  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4fee5ab7-60a7-4f5e-adfc-98f51758ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# create the data\n",
    "\n",
    "X = housing_df.drop(\"MedHouseVal\", axis=1)\n",
    "Y = housing_df[\"MedHouseVal\"]\n",
    "\n",
    "\n",
    "# Split into training ans test sets.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "\n",
    "# Create the model instance\n",
    "# Create a Random Forest Regressor model with 10 decision trees\n",
    "# The RandomForestRegressor is an ensemble learning method that builds a forest of decision trees\n",
    "# n_estimators is a hyperparameter that determines the number of decision trees in the forest\n",
    "# Setting n_estimators=10 means the model will use 10 decision trees for making predictions\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100) # default n_estimatrors is = 100\n",
    "\n",
    "\n",
    "# Train (fit) the RandomForestRegressor model on the training data\n",
    "# X_train: Input features of the training data\n",
    "# Y_train: Target values corresponding to the training data\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Use the trained RandomForestRegressor model to make predictions on the test data\n",
    "# X_test: Input features of the test data\n",
    "# Y_preds: Predicted target values based on the input features\n",
    "\n",
    "Y_preds = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "45dd7194-5288-436f-adcb-a7fadc9e23f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49384  , 0.75494  , 4.9285964, 2.54316  , 2.33176  , 1.6525301,\n",
       "       2.34323  , 1.66182  , 2.47489  , 4.8344779])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 10 predicted target values from the RandomForestRegressor model\n",
    "# Y_preds[:10]: Subset of predicted target values for the first 10 samples in the test data\n",
    "Y_preds[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6b411b7c-d4b9-44fe-bfe8-a0e24432dc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.477  , 0.458  , 5.00001, 2.186  , 2.78   , 1.587  , 1.982  ,\n",
       "       1.575  , 3.4    , 4.466  ])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the true target values of the first 10 samples in the test data to a NumPy array\n",
    "# np.array(Y_test[:10]): Creating a NumPy array from the true target values for comparison\n",
    "np.array(Y_test[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b6b40f62-1486-40f6-b76b-b41a938a11b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4128"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a39e20dc-9000-41ac-b27a-2a3d19b80792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4128"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2be43193-a357-487a-b9a2-fcd2fb7fabf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32659871732073664"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the mean_absolute_error function to calculate the mean absolute error (MAE)\n",
    "# MAE is a metric that measures the average absolute difference between the true and predicted values\n",
    "# It provides a straightforward way to understand the average magnitude of errors in the predictions\n",
    "# Lower MAE indicates better model performance, with 0 being a perfect match\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(Y_test, Y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "78834f12-4d47-4b4b-8a56-a36203d43e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        4.526\n",
       "1        3.585\n",
       "2        3.521\n",
       "3        3.413\n",
       "4        3.422\n",
       "         ...  \n",
       "20635    0.781\n",
       "20636    0.771\n",
       "20637    0.923\n",
       "20638    0.847\n",
       "20639    0.894\n",
       "Name: MedHouseVal, Length: 20640, dtype: float64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df[\"MedHouseVal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "46af7879-f66b-429f-853d-eb1a9e9dd621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0. An end -to-end Scikit learn workfolw.',\n",
       " '1. Getting the data ready.',\n",
       " '2. Choose the right estimator/algorith for our problems.',\n",
       " '3. Fit the model/algorithm and use it to make predictions on our data',\n",
       " '4. Evaluate a model.',\n",
       " '5. Improve a model.',\n",
       " '6. Save and Load a trained model.',\n",
       " '7. Putting it all together!']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "What_we_are_going_to_cover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89adf144-171b-4403-a4ec-82c73b61e3ee",
   "metadata": {},
   "source": [
    "## Evaluating a machine learning model \n",
    "\n",
    "Three ways to evaluate Scikit-learn models:\n",
    "\n",
    "     1. Estimator's built-in `score()`  method\n",
    "     2. The `scoring` parameter\n",
    "     3. Problem-specific metric functions.\n",
    "\n",
    "\n",
    "You can find more information here: https://scikit-learn.org/stable/modules/model_evaluation.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d589cf-8495-48ab-ae15-3ebd504f6bfb",
   "metadata": {},
   "source": [
    "### Evaluating a model with the `score` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ec29065f-2c65-4aac-a83a-2fe75b3d2a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Create X & Y\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "Y = heart_disease[\"target\"]\n",
    "\n",
    "\n",
    "# Create train/test\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2)\n",
    "\n",
    "# Instantiate Random Forest Classifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "\n",
    "# Fit classifer to training data\n",
    "clf.fit (X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3a9fac13-87c3-4700-82f3-e882c8180638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The highest value for the .score() method is 1.0 the lowest is 0.0\n",
    "# The model is geting a 100% score as it has seen all the training data.\n",
    "clf.score(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "702a619a-934d-4284-8ac9-ab066114a11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8524590163934426"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will be the score on the unseed data \n",
    "clf.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2bd5bb-dd8e-4a2c-aff4-db77cf18d0eb",
   "metadata": {},
   "source": [
    "Lets try the `score()` method  on a regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "00c85835-433e-4109-83b8-c58e0e97e4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# create the data\n",
    "\n",
    "X = housing_df.drop(\"MedHouseVal\", axis=1)\n",
    "Y = housing_df[\"MedHouseVal\"]\n",
    "\n",
    "\n",
    "# Split into training ans test sets.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "\n",
    "# Create the model instance\n",
    "# Create a Random Forest Regressor model with 10 decision trees\n",
    "# The RandomForestRegressor is an ensemble learning method that builds a forest of decision trees\n",
    "# n_estimators is a hyperparameter that determines the number of decision trees in the forest\n",
    "# Setting n_estimators=10 means the model will use 10 decision trees for making predictions\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100) # default n_estimatrors is = 100\n",
    "\n",
    "\n",
    "# Train (fit) the RandomForestRegressor model on the training data\n",
    "# X_train: Input features of the training data\n",
    "# Y_train: Target values corresponding to the training data\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bd2e9b17-1272-437d-bd95-4709908ebd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8065734772187598"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the performance of the regression model 'model' on the test dataset.\n",
    "# 'X_test' contains the test features, and 'Y_test' contains the true labels.\n",
    "# For regression algorithms, the default metric used by 'score()' is R-squared (r_squared).\n",
    "# R-squared values range from 0.0 (worst) to 1.0 (best), indicating the proportion of \n",
    "# variance for the dependent variable that's explained by the independent variables in the model.\n",
    "model.score(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6691671b-0ab4-43c0-a165-0b8e82813488",
   "metadata": {},
   "source": [
    "## Evaluating a model using the `score` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "28af6219-8d25-4a44-8bf4-44b03521f63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries for machine learning\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Set a random seed for reproducibility of results\n",
    "np.random.seed(42)\n",
    "\n",
    "# Prepare the feature matrix (X) by dropping the target variable column\n",
    "X = housing_df.drop(\"MedHouseVal\", axis=1)  # 'MedHouseVal' is the target variable\n",
    "\n",
    "# Prepare the target vector (Y) by selecting only the target column\n",
    "Y = housing_df[\"MedHouseVal\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# 80% of the data is used for training and 20% for testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# Initialize the RandomForestRegressor model\n",
    "# Setting n_estimators to 100, specifying the number of trees in the forest\n",
    "clf = RandomForestRegressor(n_estimators=100)  # Default n_estimators is 100\n",
    "\n",
    "# Fit the RandomForestRegressor model on the training data\n",
    "# This step involves the model learning the relationship between features and the target variable\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# After this, clf is now a trained RandomForestRegressor model\n",
    "# It can be used to make predictions and evaluate its performance on unseen data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e9b2a4d0-7476-4a7f-ba66-183bb7b75bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8065734772187598"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the accuracy of the classifier 'clf' on the test dataset\n",
    "# 'X_test' contains the test features, and 'Y_test' contains the true labels.\n",
    "# The 'score' method compares the model's predictions with the true labels \n",
    "# to calculate the accuracy (for classification) or R^2 score (for regression).\n",
    "clf.score(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "443e876d-bace-4d64-85fa-1c7e1f08639f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50097272, 0.70273041, 0.74115332, 0.61556377, 0.68253631])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform cross-validation on the classifier 'clf' using the dataset 'X' and labels 'Y'.\n",
    "# The 'cross_val_score' function splits 'X' and 'Y' into multiple subsets (folds), \n",
    "# and for each fold, it trains the classifier 'clf' on the fold's training set \n",
    "# and evaluates its performance on the fold's test set.\n",
    "# Here, 'cv=5' specifies that the data should be split into 5 folds, \n",
    "# providing a balance between training and testing each subset.\n",
    "# This approach helps in assessing the model's effectiveness and \n",
    "# generalization capability over different data samples.\n",
    "cross_val_score(clf, X, Y, cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f5db724c-4324-4b51-babd-979042dc03f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8065734772187598, 0.6501848621545904)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Set a random seed for reproducibility of the experiment\n",
    "np.random.seed(420)\n",
    "\n",
    "# Evaluating the RandomForestRegressor model on a single training/test split\n",
    "# This gives us an idea of how the model performs on this particular partition of the data\n",
    "clf_single_score = clf.score(X_test, Y_test)\n",
    "\n",
    "# Evaluating the model using 5-fold cross-validation\n",
    "# This method splits the entire dataset into 5 parts, trains the model 5 times using 4 parts as training data\n",
    "# and the remaining part as testing data each time, and calculates the score for each iteration.\n",
    "# Taking the mean of these scores gives us a more robust estimate of the model's performance.\n",
    "clf_cross_val_score = np.mean(cross_val_score(clf, X, Y, cv=5))\n",
    "\n",
    "# Compare the two scores to understand how the model's performance might vary\n",
    "# between a single split and across different folds of cross-validation.\n",
    "# A significant difference might indicate the model's sensitivity to the data partitioning.\n",
    "(clf_single_score, clf_cross_val_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a6465e-5b3a-4d59-afe1-d10651e5215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring parameter set to None by default\n",
    "# Default scoring parameter of classifier = mean accuracy\n",
    "\n",
    "# Perform cross-validation on the classifier 'clf' using the dataset 'X' and labels 'Y'.\n",
    "# The 'cross_val_score' function with 'scoring=None' defaults to the classifier's \n",
    "# standard scoring method, which for most classifiers is mean accuracy.\n",
    "# 'cv=5' specifies that the data is split into 5 subsets (folds).\n",
    "# In each of the 5 iterations, the classifier is trained on 4 folds and tested on the remaining fold.\n",
    "# This provides a robust evaluation of the classifier's performance across different subsets of the data.\n",
    "cross_val_score(clf, X, Y, cv=5, scoring=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f89c497-ad3d-4142-86c6-18b9f82373f0",
   "metadata": {},
   "source": [
    "### Classification model evaluation metrics\n",
    "\n",
    "1. Accuracy \n",
    "2. Area under ROC curve\n",
    "3. Confusion matrix\n",
    "4. Classification report\n",
    "\n",
    "\n",
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbbd32d-8586-4494-b615-c83526d3b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions and models from scikit-learn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Ensuring reproducibility of results by setting a random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# with 'target' as the column indicating the presence or absence of heart disease.\n",
    "# Preparing the feature matrix (X) and target vector (Y)\n",
    "X = heart_disease.drop(\"target\", axis=1)  # Features: all columns except 'target'\n",
    "Y = heart_disease[\"target\"]  # Target: the 'target' column\n",
    "\n",
    "# Instantiate a RandomForestClassifier with 50 trees\n",
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "# Perform 5-fold cross-validation to evaluate the model\n",
    "# The function cross_val_score() automatically splits the data into folds,\n",
    "# trains the model on the training folds, and tests it on the testing fold for each split.\n",
    "# cv=5 specifies that the data should be split into 5 parts for cross-validation.\n",
    "cross_val_scores = cross_val_score(clf, X, Y, cv=5)\n",
    "\n",
    "# cross_val_scores now contains the accuracy scores of the model for each fold.\n",
    "# You can further analyze these scores (e.g., calculate the mean) to estimate the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f74f6f-0676-4742-a582-32c1de1cb034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the cross-validation scores\n",
    "mean_cross_val_score = np.mean(cross_val_scores)\n",
    "\n",
    "# Print the mean cross-validation score\n",
    "print(\"Mean cross-validation score:\", mean_cross_val_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6989276-818f-4085-b580-957b90acf7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Heart Disease Classifier Cross-validated Accuracy: {np.mean(cross_val_scores) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9914e7-c622-4a03-8e8f-78ff7ee38889",
   "metadata": {},
   "source": [
    "**Area under the reciever operating characteristic curve(AUC/ROC)**\n",
    "\n",
    "\n",
    "* Area under curve (AUC)\n",
    "*ROC curve\n",
    "\n",
    "ROC curves are a comparison of a model's true postive rate (tpr) versus a model false\n",
    "positive rate (fpr).\n",
    "\n",
    "* True positive = model predicts 1 when truth is 1\n",
    "* False positive = model predicts 1 when truth is 0 \n",
    "* True negative = model predicts 0 when truth is 0\n",
    "* False negative = model predicts 0 when truth is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f505e2ca-4307-4109-8aa1-35b75d4c3ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets.\n",
    "# 'train_test_split' randomly divides the data into two parts:\n",
    "# 1. Training set (X_train and Y_train) used for the model to learn from.\n",
    "# 2. Testing set (X_test and Y_test) used to evaluate the model's performance.\n",
    "# The 'test_size=0.2' argument specifies that 20% of the data should be held out for testing,\n",
    "# meaning the remaining 80% is used for training the model.\n",
    "# This function ensures that your model is tested on unseen data, simulating real-world predictions.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# After this step, you have four subsets of data:\n",
    "# X_train: The feature variables used for training.\n",
    "# Y_train: The target variable corresponding to X_train, used for training.\n",
    "# X_test: The feature variables set aside for testing the model.\n",
    "# Y_test: The target variable corresponding to X_test, used to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "973059a6-ae31-4b52-8a77-1f7c312c6b76",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomForestRegressor' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[159], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Make predictions with probabilities on the test set\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# The predict_proba method returns the probability estimates for all classes,\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# which is useful for evaluating the classifier's performance through metrics like ROC.\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m Y_probs \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m(X_test)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Display the first 10 prediction probabilities and the total number of predictions\u001b[39;00m\n\u001b[0;32m     13\u001b[0m Y_probs[:\u001b[38;5;241m10\u001b[39m], \u001b[38;5;28mlen\u001b[39m(Y_probs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomForestRegressor' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "# This step trains the model using the features (X_train) and the target (Y_train).\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions with probabilities on the test set\n",
    "# The predict_proba method returns the probability estimates for all classes,\n",
    "# which is useful for evaluating the classifier's performance through metrics like ROC.\n",
    "Y_probs = clf.predict_proba(X_test)\n",
    "\n",
    "# Display the first 10 prediction probabilities and the total number of predictions\n",
    "Y_probs[:10], len(Y_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b0b93-b4e0-434e-80ba-ac587f698b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract positive class probabilities\n",
    "# The [:,1] indexing selects the probabilities of the positive class (usually '1') for all instances.\n",
    "# This is typically required for metrics like ROC AUC, where the focus is on the performance concerning the positive class.\n",
    "Y_probs_positive = Y_probs[:, 1]\n",
    "\n",
    "# Display the first 10 positive class probabilities\n",
    "Y_probs_positive[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f695c987-8910-4db4-ab0e-dd7cbc7a257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Calculate False Positive Rate (fpr), True Positive Rate (tpr), and thresholds\n",
    "# The roc_curve function computes the Receiver Operating Characteristic (ROC) curve,\n",
    "# which illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n",
    "# It's a plot of the True Positive Rate (tpr) against the False Positive Rate (fpr) at various threshold settings.\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, Y_probs_positive)\n",
    "\n",
    "# Display the False Positive Rates (fpr)\n",
    "# The fpr array contains the false positive rates for each threshold used to compute the ROC curve.\n",
    "# A false positive rate indicates how often the model incorrectly classifies a negative observation as positive.\n",
    "fpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7152975-fdba-4da2-ae76-6f1f6dc4ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "    \"\"\"\n",
    "    Plots the Receiver Operating Characteristic (ROC) curve.\n",
    "    \n",
    "    The ROC curve visualizes the performance of a classification model by plotting\n",
    "    the true positive rate (TPR) against the false positive rate (FPR) at various threshold levels.\n",
    "    \n",
    "    Parameters:\n",
    "    - fpr: Array of false positive rates.\n",
    "    - tpr: Array of true positive rates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plot the ROC curve using the FPR and TPR values\n",
    "    plt.plot(fpr, tpr, color=\"orange\", label=\"ROC\")\n",
    "    \n",
    "    # Plot a diagonal dashed line representing a no-skill classifier (baseline)\n",
    "    plt.plot([0, 1], [0, 1], color=\"darkblue\", linestyle=\"--\", label=\"No Skill\")\n",
    "    \n",
    "    # Add labels and title for clarity\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    \n",
    "    # Display the legend to differentiate between the ROC curve and the baseline\n",
    "    plt.legend()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage of the function (assuming fpr and tpr are defined from a model's predictions)\n",
    "plot_roc_curve(fpr, tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f541fb7-a9a6-45ad-9c92-0880ad9ecc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Calculate the ROC AUC Score\n",
    "# The roc_auc_score function computes the area under the ROC curve,\n",
    "# which represents the model's ability to discriminate between positive and negative classes.\n",
    "# A higher score indicates better performance, with 1.0 representing a perfect model.\n",
    "roc_auc = roc_auc_score(Y_test, Y_probs_positive)\n",
    "\n",
    "# Print the ROC AUC score to evaluate the model\n",
    "print(f\"ROC AUC Score: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb07456-1b68-4a6d-92a1-7975a6e63dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot perfect ROC cueve and AUC score\n",
    "fpr, tpr, thresholds =roc_curve(Y_test, Y_test)\n",
    "plot_roc_curve(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3750cb53-5a14-4644-abfd-6c3651fc6749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfect AUC score is \n",
    "roc_auc_score(Y_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f861beb3-58c9-4dd5-97ef-4361d51cd949",
   "metadata": {},
   "source": [
    "**Confusion Matrix**\n",
    "\n",
    "A confusion matrix serves as a rapid tool for comparing the predicted labels of a model with the actual labels it was expected to predict. Essentially, it provides insights into the areas where the model encounters confusion, offering a clear visualization of its performance across different classes or categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6075299-68f0-4729-8aef-769b5e0399be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More info on confusion matrices:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "# Importing the confusion_matrix function from sklearn.metrics module\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Making predictions using the trained classifier (clf) on the test data (X_test)\n",
    "Y_preds = clf.predict(X_test)\n",
    "\n",
    "# Generating the confusion matrix to evaluate the performance of the classifier\n",
    "# The confusion matrix compares the actual labels (Y_test) with the predicted labels (Y_preds)\n",
    "# It provides insights into the classifier's performance in terms of true positives, false positives,\n",
    "# true negatives, and false negatives across different classes.\n",
    "confusion_matrix(Y_test, Y_preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d225f-c64a-46ac-8b88-633928ce4dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing confusion matrix with pd.crosstab()\n",
    "\n",
    "# Importing pandas library for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Creating a cross-tabulation (crosstab) of actual vs. predicted labels\n",
    "# The crosstab function computes a simple cross-tabulation of two (or more) factors.\n",
    "# It computes a frequency table of the factors, which in this case are the actual labels (Y_test) and predicted labels (Y_preds).\n",
    "# The rownames and colnames parameters are used to specify the names for the rows and columns of the resulting DataFrame,\n",
    "# which correspond to the actual and predicted labels, respectively.\n",
    "confusion_matrix_df = pd.crosstab(Y_test,\n",
    "                                   Y_preds,\n",
    "                                   rownames=[\"Actual Labels\"],\n",
    "                                   colnames=[\"Predicted Labels\"]\n",
    "                                  )\n",
    "\n",
    "# Displaying the cross-tabulated confusion matrix\n",
    "confusion_matrix_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47f13e-30f7-4c18-bf1e-aa1a4657505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "22 + 7 + 8 + 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68a32c-5f32-474a-8ccb-1e1fd6bea6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7deab4e-2966-42ce-b619-8e5b68aacb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing a Conda package into the current environment from a Jupyter Notebook\n",
    "import sys\n",
    "\n",
    "# The following command installs the 'seaborn' package into the current Conda environment.\n",
    "# '--yes' flag confirms the installation without user confirmation.\n",
    "# '--prefix {sys.prefix}' specifies the path to the current Conda environment.\n",
    "!conda install --yes --prefix {sys.prefix} seaborn \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13fa444-6fa8-4c7f-ba61-b6daf751059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the confusion matrix more visual using Seaborn's heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the font scale for better readability\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_mat = confusion_matrix(Y_test, Y_preds)\n",
    "\n",
    "# Plot the confusion matrix using Seaborn's heatmap\n",
    "plt.figure(figsize=(10, 8))  # Adjust the figure size if needed\n",
    "sns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"Actual Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a241f41-45c2-4380-9047-d4b94cfdcf84",
   "metadata": {},
   "source": [
    "### Creating a confussion matrix using scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef50f3c-93c4-488e-ae62-f50376838298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99da6f-5d46-40ef-abef-97f23c23bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b025f62-8c14-4917-8004-3bd5de202b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the ConfusionMatrixDisplay class from sklearn.metrics module\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Generating predictions using the classifier (clf) on the features (X)\n",
    "Y_preds = clf.predict(X)\n",
    "\n",
    "# Creating a ConfusionMatrixDisplay object from the predictions and true labels\n",
    "# The ConfusionMatrixDisplay class provides a way to visualize confusion matrices.\n",
    "# It requires predicted labels (Y_preds) and true labels (Y) as input.\n",
    "# Since we already have the predicted labels (Y_preds), we can directly use them.\n",
    "# Note: Make sure the classifier (clf) has been trained and fitted before making predictions.\n",
    "confusion_matrix_display = ConfusionMatrixDisplay.from_predictions(y_true=Y, y_pred=Y_preds)\n",
    "\n",
    "# Displaying the confusion matrix\n",
    "confusion_matrix_display.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e1deba-31dd-4c7a-b7c7-3df5f8edaac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Perform cross-validation predictions\n",
    "y_pred = cross_val_predict(clf, X, Y, cv=5)\n",
    "\n",
    "# Import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Create ConfusionMatrixDisplay object\n",
    "confusion_matrix_display = ConfusionMatrixDisplay.from_predictions(y_true=Y, y_pred=y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "confusion_matrix_display.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd48777-0a52-4af5-a957-5d34c37434bc",
   "metadata": {},
   "source": [
    "**Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c2852-b9fe-468d-9292-9751800db107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of Y_test and Y_preds\n",
    "print(\"Shape of Y_test:\", Y_test.shape)\n",
    "print(\"Shape of Y_preds:\", Y_preds.shape)\n",
    "\n",
    "# Check the length of Y_test and Y_preds\n",
    "print(\"Length of Y_test:\", len(Y_test))\n",
    "print(\"Length of Y_preds:\", len(Y_preds))\n",
    "\n",
    "# If the lengths don't match, print the first few elements to identify any discrepancies\n",
    "if len(Y_test) != len(Y_preds):\n",
    "    print(\"First 10 elements of Y_test:\", Y_test[:10])\n",
    "    print(\"First 10 elements of Y_preds:\", Y_preds[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f238e-97bf-40d0-a879-74a37cb1418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model using the training data (X_train and Y_train)\n",
    "# For example, if 'clf' is a classifier model, fit it to the training data:\n",
    "# clf.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the test data (X_test) using the trained model\n",
    "# Store the predicted labels in Y_preds\n",
    "Y_preds = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model by generating a classification report\n",
    "# The classification report provides precision, recall, F1-score, and support for each class\n",
    "# Compare the true labels (Y_test) with the predicted labels (Y_preds)\n",
    "print(classification_report(Y_test, Y_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e06ed-2cf1-4e18-a18c-b3cbf7c1c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where precision and recall become valuable\n",
    "\n",
    "disease_true = np.zeros(10000)\n",
    "disease_true[0] = 1 # only one positive case\n",
    "\n",
    "disease_preds = np.zeros(10000) # model predicts every case as 0\n",
    "\n",
    "pd.DataFrame(classification_report(disease_true,disease_preds,output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a7d351-2777-4770-8e26-380100e1eb3e",
   "metadata": {},
   "source": [
    "To summerize clasiification metrics:\n",
    "\n",
    "* **Accuracy** is a good measure to start with if all class are balanced (e.g. same amount of samples which are labelledd with 0 and 1)\n",
    "*  **Percision** and **Recall** become more important when classed are imbalanced.\n",
    "*  If false positives predictions are worse than false negatives, aim for higher percision.\n",
    "*  If false negatives predictions are worse than false postive, aim for higher call.\n",
    "*   **F1-score** is a combination of percision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f848954a-c279-44d7-9752-59740f0a0116",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9299cb39-33bd-4508-a172-4c9c04e4622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = housing_df.drop(\"MedHouseVal\", axis=1)\n",
    "Y = housing_df[\"MedHouseVal\"]\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "\n",
    "model= RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659c706-ce61-458d-aa00-8e27ee5b1e94",
   "metadata": {},
   "source": [
    "### Regression model evaluation metrics\n",
    "Model evaluation metrics documentation - https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n",
    "\n",
    "Let's go through a few of them:\n",
    "\n",
    "*  RÂ² score, the coefficient of determination \n",
    "*  Mean absolute error (MAE)\n",
    "*  Mean squared error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b2706-7781-4b71-88a0-a64d41ec9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# create the data\n",
    "\n",
    "X = housing_df.drop(\"MedHouseVal\", axis=1)\n",
    "Y = housing_df[\"MedHouseVal\"]\n",
    "\n",
    "\n",
    "# Split into training ans test sets.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "\n",
    "# Create the model instance\n",
    "# Create a Random Forest Regressor model with 10 decision trees\n",
    "# The RandomForestRegressor is an ensemble learning method that builds a forest of decision trees\n",
    "# n_estimators is a hyperparameter that determines the number of decision trees in the forest\n",
    "# Setting n_estimators=10 means the model will use 10 decision trees for making predictions\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100) # default n_estimatrors is = 100\n",
    "\n",
    "\n",
    "# Train (fit) the RandomForestRegressor model on the training data\n",
    "# X_train: Input features of the training data\n",
    "# Y_train: Target values corresponding to the training data\n",
    "\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd096bd-fbdf-482b-a63b-85fb8582625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07635b-d164-48e2-af67-7bc278fc5834",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a4a36-34b3-4dca-a22b-9bf8a5407081",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a201a5-9b76-4fa9-853e-fd414bfe7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the target variable in the test set (Y_test).\n",
    "Y_test_mean = Y_test.mean()  # Calculate the mean of Y_test\n",
    "\n",
    "# The mean of Y_test can provide insights into the distribution of the target variable.\n",
    "# For binary classification problems, where the target variable represents a binary outcome (e.g., presence or absence of a condition),\n",
    "# the mean can indicate the prevalence of the positive class (e.g., the proportion of positive instances in the test set).\n",
    "# For example, in a binary classification problem where 1 represents the positive class and 0 represents the negative class,\n",
    "# the mean of Y_test can give the proportion of positive instances in the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee30834-5ecd-4ea6-9669-9d0739c4b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the r2_score function from the sklearn.metrics module\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Creating an array filled with the mean of Y_test\n",
    "Y_test_mean = np.full(len(Y_test), Y_test.mean())\n",
    "\n",
    "# The r2_score function calculates the coefficient of determination (R^2) regression score.\n",
    "# It measures the proportion of the variance in the dependent variable (Y_test) that is predictable from the independent variable (Y_preds).\n",
    "# R^2 score ranges from 0 to 1, where 1 indicates a perfect fit.\n",
    "# Comparing the predictions (Y_preds) with the mean of Y_test can help evaluate the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be618deb-8d38-405c-b300-5ed11ad78c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_mean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18f4d65-231d-4c01-ad19-fee07c77f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the R-squared score:\n",
    "# - `y_true=Y_test` specifies the actual values from the test dataset.\n",
    "# - `y_pred=Y_test_mean` uses the mean of Y_test values as the prediction.\n",
    "# This score evaluates how close the mean predictions are to the actual values.\n",
    "r2_score(y_true=Y_test, y_pred=Y_test_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe51c607-30ba-4a70-9bad-9fd554b75e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the R-squared score where both the true values and predicted values are `Y_test`.\n",
    "# Since `y_true` and `y_pred` are the same, the R-squared score will be 1.0,\n",
    "# indicating a perfect fit. This is expected in this scenario because the \"predictions\" are exactly the same as the actual values.\n",
    "r2_score(y_true=Y_test, y_pred=Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe5dff-97a8-46ff-aee6-8700046ab75e",
   "metadata": {},
   "source": [
    "**Mean absolute error (MAE)**\n",
    "\n",
    "MAE is the average of the absolute differences between predictions and actual values.\n",
    "\n",
    "It quantifies the average magnitude of the errors in a set of predictions, without considering their direction. \n",
    "\n",
    "MAE provides a measure of how wrong the model's predictions are; the lower the MAE, the better the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c0047-52d7-44c8-a2b5-9c813748ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Mean Absolute Error (MAE) between the actual values (Y_test) and the predicted values (Y_preds).\n",
    "# 1. `Y_preds = model.predict(X_test)`: Predict the Y values using the model and test features (X_test).\n",
    "# 2. `mean_absolute_error(Y_test, Y_preds)`: Calculate the MAE, which quantifies the average magnitude of errors between the predicted and actual values, without considering their direction.\n",
    "# The result is stored in the variable `mae`.\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "Y_preds = model.predict(X_test)\n",
    "mae = mean_absolute_error(Y_test, Y_preds)  \n",
    "print(mae)  # Assuming you want to print the MAE value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4af940-ddcd-4184-b02d-a64d45452044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with actual and predicted values:\n",
    "# 1. `data`: a dictionary with keys 'actual values' and 'Predicted values', corresponding to Y_test and Y_preds.\n",
    "# 2. Calculate the differences between 'Predicted values' and 'actual values' for each row and store in a new column 'differences'.\n",
    "# 3. Display the first 10 rows of the DataFrame to inspect the actual values, predicted values, and their differences.\n",
    "\n",
    "df = pd.DataFrame(data={\"actual values\": Y_test,\n",
    "                        \"Predicted values\": Y_preds})\n",
    "df[\"differences\"] = df[\"Predicted values\"] - df[\"actual values\"]  # Ensure column names match when performing operations.\n",
    "df.head(10)  # Display the first 10 rows of the DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86cc14-40e2-4065-b5a9-e487c9d42459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Mean Absolute Error (MAE) manually:\n",
    "# 1. Compute the absolute value of differences between predicted and actual values.\n",
    "# 2. Take the mean of these absolute differences.\n",
    "# This operation provides the average magnitude of errors between the predictions and actual values, \n",
    "# serving as a measure of model accuracy.\n",
    "\n",
    "mean_absolute_error = np.abs(df[\"differences\"]).mean()\n",
    "print(mean_absolute_error)  # Print the calculated MAE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c3e951-2a10-4e6f-b64c-645764c72ff5",
   "metadata": {},
   "source": [
    "**Mean Squared error (MSE)**\n",
    "* Mean Squared Error (MSE)\n",
    "* MSE is the mean of the square of the errors between actual and predicted values.\n",
    "* It quantifies the difference between the predicted values and the actual values, \n",
    "* providing a measure of the model's accuracy. The lower the MSE, the better the model's performance.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d010308-b581-40a9-b849-a8cfb9905a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "Y_preds = model.predict(X_test)\n",
    "mse = mean_squared_error(Y_test, Y_preds)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe0435b-1126-4cca-8c1a-73d4e2cdeca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"squared_differences\"] = np.square(df[\"differences\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039f45c-4978-4943-ae2c-4f50570948db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean Squared Error (MSE) Manually\n",
    "# 1. Square the differences: This step computes the square of each difference between \n",
    "#    actual and predicted values. Squaring the differences serves to eliminate negative \n",
    "#    values and emphasize larger errors.\n",
    "squared = np.square(df[\"differences\"])\n",
    "# 2. Calculate the mean of squared differences: The final step averages these squared \n",
    "#    differences to obtain the MSE. MSE provides a measure of the model's performance \n",
    "#    by quantifying the average squared deviation from the actual values, highlighting \n",
    "#    the accuracy of predictions.\n",
    "squared.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacb19eb-5231-4c19-b789-4e6137a107ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large_error = df.copy()\n",
    "df_large_error.iloc[0][\"squared_differences\"]= 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a18bf2-985b-459d-97db-daeea71883fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb3c033-49c1-4119-8ebb-5bfd77dc1231",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large_error.iloc[1:100] = 20\n",
    "df_large_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d208f-8845-4cc0-b92a-d5cb6faa64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE for Predictions with Larger Errors\n",
    "# This line computes the Mean Squared Error (MSE) by averaging the squared differences \n",
    "# between actual and predicted values stored in the 'squared_differences' column of \n",
    "# the dataframe 'df_large_error'. This approach is particularly useful for analyzing \n",
    "# the impact of larger prediction errors on the model's accuracy, as MSE emphasizes \n",
    "# larger errors more than smaller ones due to the squaring operation. The result is \n",
    "# a single value that represents the average squared deviation, highlighting how much \n",
    "# the predictions deviate, on average, from the actual values, especially for cases \n",
    "# with significant errors.\n",
    "mse_large_error = df_large_error[\"squared_differences\"].mean()\n",
    "print(mse_large_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab8171-2e38-4199-a557-dfb2ccdae02c",
   "metadata": {},
   "source": [
    "# Using the scoring parameter\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c54bb504-261d-4d08-b5fd-22985f98dad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cross-validation and random forest classifier from scikit-learn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Ensure reproducibility of random operations\n",
    "np.random.seed(42)\n",
    "\n",
    "# Prepare features and target variable\n",
    "X = heart_disease.drop(\"target\", axis=1)  # Features: all columns except 'target'\n",
    "Y = heart_disease[\"target\"]  # Target: 'target' column\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9c5e9611-9845-4621-9f75-929146547a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81967213, 0.90163934, 0.83606557, 0.78333333, 0.78333333])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seed for the random number generator to ensure reproducible results\n",
    "np.random.seed(42)\n",
    "\n",
    "# Perform 5-fold cross-validation to assess the RandomForestClassifier's accuracy\n",
    "# - clf: the RandomForestClassifier model instance\n",
    "# - X: feature variables of the dataset\n",
    "# - Y: target variable ('target') of the dataset\n",
    "# - cv=5: splits the data into 5 distinct sets to train and test the model 5 times\n",
    "# - scoring=None: defaults to the classifier's score method, which is accuracy for classification tasks.\n",
    "# This process helps in evaluating the model's performance by calculating the accuracy across 5 different training and testing splits.\n",
    "cv_acc = cross_val_score(clf, X, Y, cv=5, scoring=None)\n",
    "cv_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f6eba5b5-896a-48e8-b475-a0d0ecf60379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross-validated accuracy is: 82.48% \n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the mean cross-validated accuracy\n",
    "# - np.mean(cv_acc): Computes the average of the accuracy scores obtained from the 5-fold cross-validation\n",
    "# - *100: Converts the mean accuracy into a percentage\n",
    "# - :.2f%: Formats the output to two decimal places\n",
    "# This statement provides a concise summary of the model's average accuracy across all cross-validation folds.\n",
    "print(f\"The cross-validated accuracy is: {np.mean(cv_acc)*100:.2f}% \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2fe82914-a4c5-41c7-8e77-09ab82b19520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81967213, 0.90163934, 0.83606557, 0.78333333, 0.78333333])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure reproducible results by setting the random number generator seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Perform 5-fold cross-validation to measure the RandomForestClassifier's accuracy\n",
    "# - scoring=\"accuracy\": Explicitly specifies to use accuracy as the scoring metric\n",
    "# This approach assesses the model's accuracy by averaging the outcomes of training and evaluating the model on 5 different data splits.\n",
    "cv_acc = cross_val_score(clf, X, Y, cv=5, scoring=\"accuracy\")\n",
    "cv_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1ba11b84-1774-44da-a351-676c29f1eb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross-validated accuracy is: 82.48% \n"
     ]
    }
   ],
   "source": [
    "# Calculate the average of the accuracy scores from the 5-fold cross-validation and print it as a percentage\n",
    "# The formatting ensures that the accuracy is displayed up to two decimal places for clarity\n",
    "print(f\"The cross-validated accuracy is: {np.mean(cv_acc)*100:.2f}% \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2ba0485a-17a1-4910-bfc7-feeef4384493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76315789, 0.90322581, 0.83870968, 0.79411765, 0.74358974])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model's precision across 5-fold cross-validation\n",
    "# Precision is the ratio of true positive predictions to the total positive predictions (true positives + false positives)\n",
    "# This metric is particularly useful when the cost of false positives is high\n",
    "cv_precision = cross_val_score(clf, X, Y, cv=5, scoring=\"precision\")\n",
    "cv_precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "802e5b7e-e3e4-44ce-9c08-e6b5a233fadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross-validated precision is: 0.81 \n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean of the precision scores obtained from the 5-fold cross-validation and print the result\n",
    "# This average gives a general understanding of how precise the model is across all folds, in identifying positive instances correctly\n",
    "print(f\"The cross-validated precision is: {np.mean(cv_precision):.2f} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "32fd626d-d24d-4746-aba8-72ee14795b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87878788, 0.84848485, 0.78787879, 0.78787879, 0.90909091])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform 5-fold cross-validation to evaluate the RandomForestClassifier's recall\n",
    "# Recall, also known as sensitivity, measures the model's ability to correctly identify all actual positives\n",
    "# This metric is crucial when the cost of missing a positive (false negative) is high\n",
    "cv_recall = cross_val_score(clf, X, Y, cv=5, scoring=\"recall\")\n",
    "cv_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4f0c7bb9-3629-493a-87fc-22cbe1554fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross-validated recall is: 0.84 \n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean of the recall scores from the 5-fold cross-validation and print the result\n",
    "# This provides an overview of the model's ability to identify all actual positives across the cross-validation folds\n",
    "print(f\"The cross-validated recall is: {np.mean(cv_recall):.2f} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5219fdf0-86b8-435b-a2db-672b1a67ace5",
   "metadata": {},
   "source": [
    " ## lets see the `scoring` Parameter  being used for a regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "aa425aa6-e18c-45f5-a2cb-9f0ffb3ae39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = housing_df.drop(\"MedHouseVal\", axis=1)\n",
    "Y = housing_df[\"MedHouseVal\"]\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "772d8303-17e3-4eb8-a572-526a2af83214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6545458023124507"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "cv_r2 = cross_val_score(model, X,Y, cv=3, scoring= None)\n",
    "np.mean(cv_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4689283d-ee33-414a-8df3-d7e2bf9f6ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62156985, 0.72075819, 0.62130937])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ce368546-c08d-441a-986b-654ad4cc8f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4626304189468596"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean Squared error\n",
    "cv_mse = cross_val_score(model,X,Y, cv =3, scoring=\"neg_mean_squared_error\")\n",
    "np.mean(cv_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "31db7ddc-ba78-4447-b264-a7855ee77256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.51816064, -0.33163551, -0.53809511])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "46d06941-aa33-4b06-a78b-6b7976fe9a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4612318847966544"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean absolute error\n",
    "cv_mae = cross_val_score(model,X,Y, cv =3, scoring=\"neg_mean_squared_error\")\n",
    "np.mean(cv_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81327d5-0afa-46b8-8892-b9501c245832",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd45386-7303-42e0-8b47-138722cd3960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
