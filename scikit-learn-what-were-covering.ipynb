{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What we're covering in the Scikit-Learn Introduction\n",
    "\n",
    "This notebook outlines the content convered in the Scikit-Learn Introduction.\n",
    "\n",
    "It's a quick stop to see all the Scikit-Learn functions and modules for each section outlined.\n",
    "\n",
    "What we're covering follows the following diagram detailing a Scikit-Learn workflow.\n",
    "\n",
    "<img src=\"../images/sklearn-workflow-title.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Standard library imports\n",
    "\n",
    "For all machine learning projects, you'll often see these libraries (Matplotlib, NumPy and pandas) imported at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enabling inline display of matplotlib plots within the Jupyter Notebook\n",
    "# '%matplotlib inline' is a magic function in IPython that renders the plots in a cell output within the Jupyter notebook itself.\n",
    "# Without this line, plots might open in a new window or not display at all.\n",
    "%matplotlib inline\n",
    "\n",
    "# Importing the matplotlib.pyplot module\n",
    "# This module provides a MATLAB-like plotting framework, which is widely used for plotting graphs and charts in Python.\n",
    "# 'plt' is a commonly used alias for matplotlib.pyplot.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing the numpy module\n",
    "# NumPy is a fundamental package for scientific computing in Python, providing support for arrays, mathematical functions, and more.\n",
    "# 'np' is a widely used abbreviation for NumPy, making it more convenient to refer to in the code.\n",
    "import numpy as np\n",
    "\n",
    "# Importing the pandas module\n",
    "# pandas is a powerful data manipulation and analysis library for Python, providing DataFrame objects for handling tabular data.\n",
    "# 'pd' is the conventional alias for pandas, used for convenience in referencing the library.\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use 2 datasets for demonstration purposes.\n",
    "* `heart_disease` - a classification dataset (predicting whether someone has heart disease or not)\n",
    "* `boston_df` - a regression dataset (predicting the median house prices of cities in Boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and Preparing Classification and Regression Datasets\n",
    "\n",
    "# Classification Dataset: Heart Disease\n",
    "\n",
    "# Loading the heart disease dataset into a pandas DataFrame.\n",
    "# The dataset is read from a CSV file located at 'heart-disease.csv'.\n",
    "# This dataset is typically used for classification tasks.\n",
    "heart_disease = pd.read_csv(\"heart-disease.csv\")\n",
    "\n",
    "# Regression Dataset: California Housing\n",
    "\n",
    "# Importing the fetch_california_housing function from sklearn.datasets.\n",
    "# This function provides access to the California housing dataset,\n",
    "# which is commonly used for regression tasks.\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Fetching the California housing dataset.\n",
    "# The dataset is loaded as a Bunch object (similar to a dictionary).\n",
    "california_housing = fetch_california_housing()\n",
    "\n",
    "# Converting the California housing dataset from a Bunch object to a pandas DataFrame.\n",
    "# The 'data' attribute contains the features, and 'feature_names' attribute provides the column names.\n",
    "# Creating a DataFrame 'california_housing_df' with features and column names.\n",
    "california_housing_df = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)\n",
    "\n",
    "# Adding the target variable to the DataFrame.\n",
    "# The 'target' attribute in the California housing dataset contains the dependent variable (housing prices).\n",
    "# It's added as a new column named 'target' in the DataFrame.\n",
    "california_housing_df[\"target\"] = california_housing.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Heart Disease Dataset into Features and Target Variable\n",
    "\n",
    "# Creating the features matrix (X) from the heart_disease DataFrame.\n",
    "# This is done by dropping the 'target' column which is our dependent variable.\n",
    "# The 'drop' method removes the specified column ('target') from the DataFrame.\n",
    "# 'axis=1' indicates that we are dropping a column, not a row.\n",
    "# The resulting DataFrame, assigned to X, contains only the independent variables (features).\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "\n",
    "# Creating the target vector (y) from the heart_disease DataFrame.\n",
    "# The target variable is what we are trying to predict or classify.\n",
    "# In this case, 'y' is the 'target' column from the heart_disease DataFrame,\n",
    "# which represents the presence or absence of heart disease.\n",
    "y = heart_disease[\"target\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Dataset into Training and Test Sets\n",
    "\n",
    "# Importing the train_test_split function from sklearn.model_selection.\n",
    "# This function is used to randomly split the dataset into training and test subsets.\n",
    "# It's a common practice in machine learning to evaluate the performance of a model.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the features matrix (X) and target vector (y) into training and test sets.\n",
    "# The 'train_test_split' function returns four subsets:\n",
    "# X_train: part of the features used for training the model.\n",
    "# X_test: part of the features used for testing the model.\n",
    "# y_train: part of the target variable corresponding to X_train, used for training.\n",
    "# y_test: part of the target variable corresponding to X_test, used for evaluating the model.\n",
    "# By default, the function splits the data into 75% for training and 25% for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pick a model/estimator (to suit your problem)\n",
    "To pick a model we use the [Scikit-Learn machine learning map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html).\n",
    "\n",
    "<img src=\"../images/sklearn-ml-map.png\" width=400/>\n",
    "\n",
    "**Note:** Scikit-Learn refers to machine learning models and algorithms as estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and Instantiating a Random Forest Classifier for Classification Tasks\n",
    "\n",
    "# Importing the RandomForestClassifier from sklearn.ensemble.\n",
    "# RandomForestClassifier is an ensemble learning method used for classification tasks.\n",
    "# It operates by constructing a multitude of decision trees at training time \n",
    "# and outputting the class that is the mode of the classes (classification) \n",
    "# of the individual trees.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Creating an instance of the RandomForestClassifier.\n",
    "# Here, 'clf' (short for 'classifier') is instantiated as a RandomForestClassifier.\n",
    "# Without any parameters, it will use the default settings of the classifier.\n",
    "# You can customize it by passing parameters like n_estimators (number of trees),\n",
    "# max_depth (maximum depth of each tree), and others, according to your dataset and task.\n",
    "clf = RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and Instantiating a Random Forest Regressor for Regression Tasks\n",
    "\n",
    "# Importing the RandomForestRegressor from sklearn.ensemble.\n",
    "# RandomForestRegressor is an ensemble learning method primarily used for regression tasks.\n",
    "# Similar to the classifier, it operates by constructing a multitude of decision trees at training time.\n",
    "# For regression tasks, it predicts the output based on the average or mean of the outputs of individual trees.\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Creating an instance of the RandomForestRegressor.\n",
    "# Here, 'model' is instantiated as a RandomForestRegressor.\n",
    "# The default settings are used for this instance, but it can be customized with various parameters.\n",
    "# Parameters like n_estimators (number of trees), max_depth (maximum depth of each tree),\n",
    "# and others can be specified to tailor the model to specific datasets and regression tasks.\n",
    "model = RandomForestRegressor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fit the model to the data and make a prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "        0, 1, 1, 0, 0, 1, 0, 1, 1, 1], dtype=int64),\n",
       " array([[0.86, 0.14],\n",
       "        [0.21, 0.79],\n",
       "        [0.21, 0.79],\n",
       "        [0.56, 0.44],\n",
       "        [0.89, 0.11],\n",
       "        [0.15, 0.85],\n",
       "        [0.17, 0.83],\n",
       "        [0.89, 0.11],\n",
       "        [0.46, 0.54],\n",
       "        [0.51, 0.49],\n",
       "        [0.58, 0.42],\n",
       "        [0.03, 0.97],\n",
       "        [0.49, 0.51],\n",
       "        [0.74, 0.26],\n",
       "        [0.89, 0.11],\n",
       "        [0.25, 0.75],\n",
       "        [0.83, 0.17],\n",
       "        [0.78, 0.22],\n",
       "        [0.92, 0.08],\n",
       "        [0.67, 0.33],\n",
       "        [0.78, 0.22],\n",
       "        [0.61, 0.39],\n",
       "        [0.31, 0.69],\n",
       "        [0.23, 0.77],\n",
       "        [0.75, 0.25],\n",
       "        [0.95, 0.05],\n",
       "        [0.44, 0.56],\n",
       "        [0.96, 0.04],\n",
       "        [0.08, 0.92],\n",
       "        [0.2 , 0.8 ],\n",
       "        [0.93, 0.07],\n",
       "        [0.09, 0.91],\n",
       "        [0.27, 0.73],\n",
       "        [0.97, 0.03],\n",
       "        [0.29, 0.71],\n",
       "        [0.15, 0.85],\n",
       "        [0.66, 0.34],\n",
       "        [0.78, 0.22],\n",
       "        [0.89, 0.11],\n",
       "        [0.25, 0.75],\n",
       "        [0.95, 0.05],\n",
       "        [0.29, 0.71],\n",
       "        [0.51, 0.49],\n",
       "        [0.93, 0.07],\n",
       "        [0.78, 0.22],\n",
       "        [0.59, 0.41],\n",
       "        [0.14, 0.86],\n",
       "        [0.49, 0.51],\n",
       "        [0.87, 0.13],\n",
       "        [0.36, 0.64],\n",
       "        [0.28, 0.72],\n",
       "        [0.44, 0.56],\n",
       "        [0.15, 0.85],\n",
       "        [0.78, 0.22],\n",
       "        [0.19, 0.81],\n",
       "        [0.54, 0.46],\n",
       "        [0.66, 0.34],\n",
       "        [0.05, 0.95],\n",
       "        [0.72, 0.28],\n",
       "        [0.91, 0.09],\n",
       "        [0.35, 0.65],\n",
       "        [0.02, 0.98],\n",
       "        [0.2 , 0.8 ],\n",
       "        [0.08, 0.92],\n",
       "        [0.57, 0.43],\n",
       "        [0.09, 0.91],\n",
       "        [0.78, 0.22],\n",
       "        [0.3 , 0.7 ],\n",
       "        [0.07, 0.93],\n",
       "        [0.98, 0.02],\n",
       "        [0.93, 0.07],\n",
       "        [0.16, 0.84],\n",
       "        [0.94, 0.06],\n",
       "        [0.07, 0.93],\n",
       "        [0.25, 0.75],\n",
       "        [0.1 , 0.9 ]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Model and Making Predictions\n",
    "\n",
    "# Fitting the Model\n",
    "# The 'fit' method is used to train the model using the training data.\n",
    "# It adjusts the parameters of the model (clf) so it best fits the data.\n",
    "# Here, clf.fit(X_train, y_train) trains the RandomForestClassifier 'clf' \n",
    "# using the features (X_train) and the target (y_train) from the training set.\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Making Standard Predictions\n",
    "# After the model is trained, you can use the 'predict' method to make predictions.\n",
    "# 'clf.predict(X_test)' uses the features from the test set (X_test) to predict the target values.\n",
    "# The predicted target values are stored in 'y_preds'.\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Making Predictions with Probabilities (specific to classification models)\n",
    "# The 'predict_proba' method is used to predict class probabilities for classification models.\n",
    "# It returns the probability of the test data belonging to each class.\n",
    "# In the case of RandomForestClassifier, it gives the mean predicted class probabilities\n",
    "# of the trees in the forest.\n",
    "# The probabilities for the test set (X_test) are stored in 'y_probs'.\n",
    "y_probs = clf.predict_proba(X_test)\n",
    "\n",
    "# Viewing Predictions and Probabilities\n",
    "# Printing 'y_preds' and 'y_probs' to see the predictions and the associated probabilities.\n",
    "# 'y_preds' shows the predicted class labels, while 'y_probs' shows the probability estimates.\n",
    "y_preds, y_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the model\n",
    "\n",
    "Every Scikit-Learn model has a default metric which is accessible through the `score()` function.\n",
    "\n",
    "However there are a range of different evaluation metrics you can use depending on the model you're using.\n",
    "\n",
    "A full list of evaluation metrics can be [found in the documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8026315789473685"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All models/estimators have a score() function\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81967213 0.91803279 0.81967213 0.81666667 0.78333333]\n",
      "[0.83333333 0.93333333 0.83870968 0.87096774 0.75675676]\n"
     ]
    }
   ],
   "source": [
    "# Evaluting a model using cross-validation is possible with cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# scoring=None means default score() metric is used\n",
    "print(cross_val_score(estimator=clf, \n",
    "                      X=X, \n",
    "                      y=y, \n",
    "                      cv=5, # use 5-fold cross-validation\n",
    "                      scoring=None)) \n",
    "\n",
    "# Evaluate a model with a different scoring method\n",
    "print(cross_val_score(estimator=clf, \n",
    "                      X=X, \n",
    "                      y=y,\n",
    "                      cv=5, # use 5-fold cross-validation\n",
    "                      scoring=\"precision\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8026315789473685\n",
      "0.8028413028413027\n",
      "[[30  7]\n",
      " [ 8 31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80        37\n",
      "           1       0.82      0.79      0.81        39\n",
      "\n",
      "    accuracy                           0.80        76\n",
      "   macro avg       0.80      0.80      0.80        76\n",
      "weighted avg       0.80      0.80      0.80        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Different classification metrics\n",
    "\n",
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_preds))\n",
    "\n",
    "# Reciver Operating Characteristic (ROC curve)/Area under curve (AUC)\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_probs[:, 1])\n",
    "print(roc_auc_score(y_test, y_preds))\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_preds))\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.8097268423304165\n",
      "Mean Absolute Error (MAE): 0.32706978820251953\n",
      "Mean Squared Error (MSE): 0.2611744394857828\n"
     ]
    }
   ],
   "source": [
    "# Different regression metrics using the California Housing Dataset\n",
    "\n",
    "# Make predictions first\n",
    "# Using the California housing dataset DataFrame for the regression model\n",
    "X = california_housing_df.drop(\"target\", axis=1)\n",
    "y = california_housing_df[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate and train the RandomForestRegressor model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using various regression metrics\n",
    "\n",
    "# R^2 (pronounced r-squared) or coefficient of determination\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R^2 Score:\", r2_score(y_test, y_preds))\n",
    "\n",
    "# Mean absolute error (MAE)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, y_preds))\n",
    "\n",
    "# Mean square error (MSE)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Improve through experimentation\n",
    "\n",
    "Two of the main methods to improve a models baseline metrics (the first evaluation metrics you get).\n",
    "\n",
    "From a data perspective asks:\n",
    "* Could we collect more data? In machine learning, more data is generally better, as it gives a model more opportunities to learn patterns.\n",
    "* Could we improve our data? This could mean filling in misisng values or finding a better encoding (turning things into numbers) strategy.\n",
    "\n",
    "From a model perspective asks:\n",
    "* Is there a better model we could use? If you've started out with a simple model, could you use a more complex one? (we saw an example of this when looking at the [Scikit-Learn machine learning map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html), ensemble methods are generally considered more complex models)\n",
    "* Could we improve the current model? If the model you're using performs well straight out of the box, can the **hyperparameters** be tuned to make it even better?\n",
    "\n",
    "**Hyperparameters** are like settings on a model you can adjust so some of the ways it uses to find patterns are altered and potentially improved. Adjusting hyperparameters is referred to as hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to find a model's hyperparameters\n",
    "clf = RandomForestClassifier()\n",
    "clf.get_params() # returns a list of adjustable hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8421052631578947\n",
      "0.8552631578947368\n"
     ]
    }
   ],
   "source": [
    "# Example of adjusting hyperparameters by hand\n",
    "\n",
    "# Split data into X & y\n",
    "X = heart_disease.drop(\"target\", axis=1) # use all columns except target\n",
    "y = heart_disease[\"target\"] # we want to predict y using X\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Instantiate two models with different settings\n",
    "clf_1 = RandomForestClassifier(n_estimators=100)\n",
    "clf_2 = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "# Fit both models on training data\n",
    "clf_1.fit(X_train, y_train)\n",
    "clf_2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate both models on test data and see which is best\n",
    "print(clf_1.score(X_test, y_test))\n",
    "print(clf_2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=6, n_estimators=1200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=6, n_estimators=1200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=6, n_estimators=1200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=6, n_estimators=1200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=6, n_estimators=1200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=4, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=1200; total time=   2.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=1200; total time=   2.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=1200; total time=   2.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=1200; total time=   2.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=1200; total time=   2.5s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=4, n_estimators=1200; total time=   2.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=4, n_estimators=1200; total time=   2.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=4, n_estimators=1200; total time=   2.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=4, n_estimators=1200; total time=   2.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=4, n_estimators=1200; total time=   2.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=1200; total time=   2.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=1200; total time=   2.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=1200; total time=   2.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=1200; total time=   2.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=1200; total time=   2.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=1200; total time=   2.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=1200; total time=   2.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=1200; total time=   2.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=1200; total time=   2.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=1200; total time=   2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "15 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\yisakg\\desktop\\sample_project_1\\env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.83477891        nan        nan 0.84319728 0.83903061        nan\n",
      " 0.85544218 0.83469388 0.85127551 0.85136054]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_estimators': 1200, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': None}\n",
      "Model score: 0.7868852459016393\n"
     ]
    }
   ],
   "source": [
    "# Example of adjusting hyperparameters computationally (recommended)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define a grid of hyperparameters\n",
    "grid = {\"n_estimators\": [10, 100, 200, 500, 1000, 1200],\n",
    "        \"max_depth\": [None, 5, 10, 20, 30],\n",
    "        \"max_features\": [\"auto\", \"sqrt\"],\n",
    "        \"min_samples_split\": [2, 4, 6],\n",
    "        \"min_samples_leaf\": [1, 2, 4]}\n",
    "\n",
    "# Ensure X and y are defined and properly formatted\n",
    "# X = ... (your features)\n",
    "# y = ... (your target variable)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_jobs=1)  # n_jobs set to 1 for compatibility\n",
    "\n",
    "# Setup RandomizedSearchCV with KFold cross-validation\n",
    "rs_clf = RandomizedSearchCV(estimator=clf,\n",
    "                            param_distributions=grid,\n",
    "                            n_iter=10,  # try 10 models total\n",
    "                            cv=KFold(n_splits=5),  # using simple 5-fold cross-validation\n",
    "                            verbose=2)  # print out results\n",
    "\n",
    "# Fit the RandomizedSearchCV version of clf\n",
    "rs_clf.fit(X_train, y_train)\n",
    "\n",
    "# Find the best hyperparameters\n",
    "print(\"Best hyperparameters:\", rs_clf.best_params_)\n",
    "\n",
    "# Scoring automatically uses the best hyperparameters\n",
    "score = rs_clf.score(X_test, y_test)\n",
    "print(\"Model score:\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save and reload your trained model\n",
    "You can save and load a model with `pickle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the 'pickle' module\n",
    "# This module is used for serializing and de-serializing Python object structures, \n",
    "# also known as 'pickling' and 'unpickling'.\n",
    "import pickle\n",
    "\n",
    "# Save an existing model to a file using pickle\n",
    "# 'pickle.dump' is used to serialize your machine learning model (here, 'rs_clf') into a file.\n",
    "# 'rs_clf' is the model you want to save. \n",
    "# 'open(\"rs_random_forest_model_1.pkl\", \"wb\")' opens a file named 'rs_random_forest_model_1.pkl' in binary write mode ('wb'). \n",
    "# If the file doesnâ€™t exist, it will be created. If it does exist, it will be overwritten.\n",
    "# The model 'rs_clf' will be saved in this file.\n",
    "pickle.dump(rs_clf, open(\"rs_random_forest_model_1.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7868852459016393"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the 'pickle' module for serializing and de-serializing Python object structures.\n",
    "import pickle\n",
    "\n",
    "# Loading a saved model using pickle\n",
    "# 'pickle.load' is used for de-serializing the Python object. In this case, it's used to load the previously saved model.\n",
    "# 'open(\"rs_random_forest_model_1.pkl\", \"rb\")' opens the file where the model is saved, in binary read mode ('rb').\n",
    "# The model saved in 'rs_random_forest_model_1.pkl' is loaded into 'loaded_pickle_model'.\n",
    "loaded_pickle_model = pickle.load(open(\"rs_random_forest_model_1.pkl\", \"rb\"))\n",
    "\n",
    "# Evaluate the loaded model\n",
    "# 'loaded_pickle_model.score(X_test, y_test)' evaluates the performance of the loaded model using the test data.\n",
    "# 'X_test' and 'y_test' are the features and labels of your test dataset, respectively.\n",
    "# This method returns the accuracy or the R^2 score of the model depending on the type of the model (classification or regression).\n",
    "loaded_pickle_model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the same with `joblib`. `joblib` is usually more efficient with numerical data (what our models are)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs_random_forest_model_1.joblib']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing 'dump' and 'load' functions from the 'joblib' library.\n",
    "# Joblib is a set of tools to provide lightweight pipelining in Python. \n",
    "# It is particularly optimized for saving and loading machine learning models, especially large models that may not fit into memory.\n",
    "from joblib import dump, load\n",
    "\n",
    "# Save a model to a file using joblib\n",
    "# The 'dump' function is used to serialize the machine learning model ('rs_clf' in this case) into a file.\n",
    "# 'rs_clf' is the model you want to save.\n",
    "# 'filename=\"gs_random_forest_model_1.joblib\"' specifies the name of the file where the model will be saved. \n",
    "# The '.joblib' extension is typically used for files saved using joblib.\n",
    "# Joblib is often more efficient for models that involve large numpy arrays, as is often the case in machine learning.\n",
    "dump(rs_clf, filename=\"gs_random_forest_model_1.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing 'load' function from the 'joblib' library.\n",
    "# Joblib is efficient for saving and loading Python objects that contain large data, especially numpy arrays.\n",
    "from joblib import load\n",
    "\n",
    "# Load a previously saved model using joblib\n",
    "# The 'load' function is used for de-serializing the saved model file back into a Python object.\n",
    "# 'filename=\"gs_random_forest_model_1.joblib\"' specifies the name of the file from which the model will be loaded.\n",
    "# The model saved in 'gs_random_forest_model_1.joblib' is loaded into 'loaded_joblib_model'.\n",
    "# This loaded model can now be used for making predictions or further analysis without the need to retrain it.\n",
    "loaded_joblib_model = load(filename=\"gs_random_forest_model_1.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7868852459016393"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing 'load' function from the 'joblib' library.\n",
    "from joblib import load\n",
    "\n",
    "# Load a previously saved model using joblib\n",
    "loaded_joblib_model = load(filename=\"gs_random_forest_model_1.joblib\")\n",
    "\n",
    "# Evaluate the performance of the loaded model using joblib\n",
    "# 'loaded_joblib_model.score(X_test, y_test)' evaluates the loaded model's performance using the test data.\n",
    "# 'X_test' and 'y_test' are the features and labels of your test dataset, respectively.\n",
    "# This method returns the accuracy or the R^2 score of the model, depending on the type of the model (classification or regression).\n",
    "# It's a quick way to assess how well the model performs on unseen data.\n",
    "loaded_joblib_model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Putting it all together (not pictured)\n",
    "\n",
    "We can put a number of different Scikit-Learn functions together using `Pipeline`.\n",
    "\n",
    "As an example, we'll use `car-sales-extended-missing-data.csv`. Which has missing data as well as non-numeric data. For a machine learning model to work, there can be no missing data or non-numeric values.\n",
    "\n",
    "The problem we're solving here is predicting a cars sales price given a number of parameters about the car (a regression problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22188417408787875"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting data ready\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Modelling\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Setup random seed\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import data and drop rows with missing target values (Price)\n",
    "data = pd.read_csv(\"car-sales-extended-missing-data.csv\")  # Update path as needed\n",
    "data.dropna(subset=[\"Price\"], inplace=True)\n",
    "\n",
    "# Define different features and transformer pipelines\n",
    "categorical_features = [\"Make\", \"Colour\"]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "door_feature = [\"Doors\"]\n",
    "door_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=4))])\n",
    "\n",
    "numeric_features = [\"Odometer (KM)\"]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))\n",
    "])\n",
    "\n",
    "# Setup preprocessing steps (fill missing values, then convert to numbers)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "        (\"door\", door_transformer, door_feature),\n",
    "        (\"num\", numeric_transformer, numeric_features)])\n",
    "\n",
    "# Create a preprocessing and modelling pipeline\n",
    "model = Pipeline(steps=[(\"preprocessor\", preprocessor),\n",
    "                        (\"model\", RandomForestRegressor())])\n",
    "\n",
    "# Split data\n",
    "X = data.drop(\"Price\", axis=1)\n",
    "y = data[\"Price\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Fit and score the model\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
